{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHbW_sqZA1S3"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load the dataset \n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)"
      ],
      "metadata": {
        "id": "NKvrxFjh-bj2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc91b89f-fcd9-42d0-f2a8-9da86af38622"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sg1dNkbU_r0u"
      },
      "outputs": [],
      "source": [
        "# no. features\n",
        "n_features = 28*28\n",
        "#no. of classes \n",
        "n_classes = 1\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nR3YOKngAx45"
      },
      "outputs": [],
      "source": [
        "#reshape the data\n",
        "x_train= x_train.reshape(-1,n_features)\n",
        "x_test= x_test.reshape(-1,n_features)\n",
        "# normalize the samples between 0 and 1\n",
        "x_train,x_test = x_train/255.0, x_test/255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VquRMIqm0wrL"
      },
      "source": [
        "V#00888037, In this code I used the last two digits of my V# as the digits to be classified."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mD3WYChlHqYC"
      },
      "outputs": [],
      "source": [
        "#function to filter the dataset and return data that has 3,7 labels as 1 and -1\n",
        "def filter_data(x,y):\n",
        " filter = (y==3) | (y==7)\n",
        " x,y = x[filter],y[filter]\n",
        " y=np.where(y == 3, 1, -1)\n",
        " return x,y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eY35hHfJXvk"
      },
      "outputs": [],
      "source": [
        "#filter data\n",
        "x_train,y_train = filter_data(x_train,y_train)\n",
        "x_test,y_test = filter_data(x_test,y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#reshape y_train \n",
        "y_train = tf.reshape(y_train, (y_train.shape[0],n_classes))\n",
        "y_test = tf.reshape(y_test, (y_test.shape[0],n_classes))"
      ],
      "metadata": {
        "id": "N9qUGESgR4m9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the training dataset.\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "# Prepare the validation dataset.\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "test_dataset = test_dataset.batch(batch_size)"
      ],
      "metadata": {
        "id": "F-zvrkjFrFtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#linear regression function\n",
        "def regression(x):\n",
        " return tf.matmul(x,w)+b"
      ],
      "metadata": {
        "id": "tUFL1HWMjImS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkdZ0829M6dz"
      },
      "outputs": [],
      "source": [
        "#logistic function(cost function)\n",
        "def loss_func (y_hat,y):\n",
        " return tf.reduce_mean(tf.math.log(1+tf.math.exp(-y*y_hat)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# L2 regularization function\n",
        "def l2_reg(cost,beta):\n",
        "  regularizer =tf.nn.l2_loss(w)\n",
        "  cost+= beta * regularizer\n",
        "  return cost"
      ],
      "metadata": {
        "id": "GCQM-crDw2Da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracy function \n",
        "def accuracy(y_hat, y):\n",
        "  positives = tf.math.logical_and(tf.math.equal(y, 1), tf.math.greater_equal(y_hat, 0))\n",
        "  negatives= tf.math.logical_and(tf.math.equal(y, -1), tf.math.less(y_hat, 0))\n",
        "  correct_predictions = tf.math.logical_or(positives,negatives)\n",
        "  return tf.reduce_mean(tf.cast(correct_predictions, tf.float32))"
      ],
      "metadata": {
        "id": "WF5tx0I45qXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJB-qmaEIOJA"
      },
      "outputs": [],
      "source": [
        "# initalize weight and bias\n",
        "w = tf.Variable(tf.random.normal([n_features, n_classes], stddev=0.01), name='W1')\n",
        "b = tf.Variable(tf.zeros([n_classes]), name='b1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THtMdRFdKw-q"
      },
      "outputs": [],
      "source": [
        "# this optimizer will do gradient descent for us\n",
        "lr = 0.001;\n",
        "optimizer = tf.optimizers.SGD(learning_rate=lr)\n",
        "#optimizer = tf.optimizers.Adam(learning_rate=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and test data in batches"
      ],
      "metadata": {
        "id": "zxZbKd7-ahx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Set training parameters\n",
        "# n_epochs = 200\n",
        "# beta =0.01\n",
        "# train_losses = []\n",
        "# test_losses = []\n",
        "# train_accuracy = []\n",
        "# test_accuracy = []\n",
        "\n",
        "# # Set up the training loop and begin training\n",
        "# for epoch in range(n_epochs):\n",
        "#   batch_train_loss = [] \n",
        "#   batch_train_acc = []\n",
        "#   batch_test_loss = [] \n",
        "#   batch_test_acc = []\n",
        "\n",
        "#   # Iterate over the training data\n",
        "#   for x_batch, y_batch in train_dataset:\n",
        "#     with tf.GradientTape() as tape:\n",
        "#       pred = regression(x_batch)\n",
        "#       loss = loss_func(pred, tf.cast(y_batch,tf.float32))\n",
        "#       loss = l2_reg(loss,beta)\n",
        "#     acc = accuracy(pred, y_batch)\n",
        "#     # Update the parameters with respect to the gradient calculations\n",
        "#     gradients = tape.gradient(loss, [w,b])\n",
        "#     optimizer.apply_gradients(zip(gradients, [w,b]))\n",
        "#     # append loss and accuracy of training batches\n",
        "#     batch_train_loss.append(loss)\n",
        "#     batch_train_acc.append(acc)\n",
        "\n",
        "#   # Iterate over the testing data\n",
        "#   for x_batch, y_batch in test_dataset:\n",
        "#     pred = regression(x_batch)\n",
        "#     loss = loss_func(pred, tf.cast(y_batch,tf.float32))\n",
        "#     acc = accuracy(pred, y_batch)\n",
        "#     # append loss and accuracy of testing batches\n",
        "#     batch_test_loss.append(loss)\n",
        "#     batch_test_acc.append(acc)\n",
        "\n",
        "#   # average the batch loss and accuracy and append it to a list\n",
        "#   train_loss=tf.reduce_mean(batch_train_loss)\n",
        "#   train_acc = tf.reduce_mean(batch_train_acc)\n",
        "#   test_loss = tf.reduce_mean(batch_test_loss)\n",
        "#   test_acc = tf.reduce_mean(batch_test_acc)\n",
        "#   train_losses.append(train_loss)\n",
        "#   train_accuracy.append(train_acc)\n",
        "#   test_losses.append(test_loss)\n",
        "#   test_accuracy.append(test_acc)\n",
        "\n",
        "#   if epoch % 10 == 0:\n",
        "#     print('Epoch: [{:}/{:}]\\t\\t-----------------------'.format(epoch, n_epochs))\n",
        "#     print('training loss: {:.3f}\\t training accuracy: {:.3f}'.format(train_loss,train_acc))\n",
        "#     print('test loss: {:.3f}\\t test accuracy: {:.3f}'.format(test_loss,test_acc))"
      ],
      "metadata": {
        "id": "7NksQVm7MsC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 5000\n",
        "beta =0.01\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_accuracy = []\n",
        "test_accuracy = []\n",
        "\n",
        "# Set up the training loop and begin training\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "  # Iterate over the training data\n",
        "    with tf.GradientTape() as tape:\n",
        "      pred = regression(x_train)\n",
        "      train_loss = loss_func(pred, tf.cast(y_train,tf.float32))\n",
        "      train_loss = l2_reg(train_loss,beta)\n",
        "    train_acc = accuracy(pred, y_train)\n",
        "    # Update the parameters with respect to the gradient calculations\n",
        "    gradients = tape.gradient(train_loss, [w,b])\n",
        "    optimizer.apply_gradients(zip(gradients, [w,b]))\n",
        "    # append loss and accuracy of training batches\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracy.append(train_acc)\n",
        "\n",
        "  # Iterate over the testing data\n",
        "    test_pred = regression(x_test)\n",
        "    test_loss = loss_func(test_pred, tf.cast(y_test,tf.float32))\n",
        "    test_acc = accuracy(test_pred, y_test)\n",
        "    # append loss and accuracy of testing batches\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracy.append(test_acc)\n",
        "    if epoch % 10 == 0:\n",
        "      print('Epoch: [{:}/{:}]-----------------------'.format(epoch, n_epochs))\n",
        "      print('training loss: {:.3f}\\t\\t training accuracy: {:.3f}'.format(train_loss,train_acc))\n",
        "      print('test loss: {:.3f}\\t\\t test accuracy: {:.3f}'.format(test_loss,test_acc))\n",
        "      "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZYCt4VmW1_G",
        "outputId": "9d6a69d2-9b61-4858-c588-297eba7ac623"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [0/5000]-----------------------\n",
            "training loss: 0.690\t\t training accuracy: 0.524\n",
            "test loss: 0.688\t\t test accuracy: 0.542\n",
            "Epoch: [10/5000]-----------------------\n",
            "training loss: 0.673\t\t training accuracy: 0.717\n",
            "test loss: 0.671\t\t test accuracy: 0.734\n",
            "Epoch: [20/5000]-----------------------\n",
            "training loss: 0.658\t\t training accuracy: 0.840\n",
            "test loss: 0.655\t\t test accuracy: 0.850\n",
            "Epoch: [30/5000]-----------------------\n",
            "training loss: 0.642\t\t training accuracy: 0.900\n",
            "test loss: 0.640\t\t test accuracy: 0.908\n",
            "Epoch: [40/5000]-----------------------\n",
            "training loss: 0.628\t\t training accuracy: 0.930\n",
            "test loss: 0.625\t\t test accuracy: 0.933\n",
            "Epoch: [50/5000]-----------------------\n",
            "training loss: 0.614\t\t training accuracy: 0.944\n",
            "test loss: 0.611\t\t test accuracy: 0.944\n",
            "Epoch: [60/5000]-----------------------\n",
            "training loss: 0.601\t\t training accuracy: 0.952\n",
            "test loss: 0.598\t\t test accuracy: 0.952\n",
            "Epoch: [70/5000]-----------------------\n",
            "training loss: 0.588\t\t training accuracy: 0.956\n",
            "test loss: 0.585\t\t test accuracy: 0.956\n",
            "Epoch: [80/5000]-----------------------\n",
            "training loss: 0.576\t\t training accuracy: 0.960\n",
            "test loss: 0.573\t\t test accuracy: 0.959\n",
            "Epoch: [90/5000]-----------------------\n",
            "training loss: 0.564\t\t training accuracy: 0.962\n",
            "test loss: 0.561\t\t test accuracy: 0.959\n",
            "Epoch: [100/5000]-----------------------\n",
            "training loss: 0.553\t\t training accuracy: 0.964\n",
            "test loss: 0.550\t\t test accuracy: 0.962\n",
            "Epoch: [110/5000]-----------------------\n",
            "training loss: 0.542\t\t training accuracy: 0.965\n",
            "test loss: 0.539\t\t test accuracy: 0.962\n",
            "Epoch: [120/5000]-----------------------\n",
            "training loss: 0.532\t\t training accuracy: 0.966\n",
            "test loss: 0.528\t\t test accuracy: 0.963\n",
            "Epoch: [130/5000]-----------------------\n",
            "training loss: 0.522\t\t training accuracy: 0.966\n",
            "test loss: 0.518\t\t test accuracy: 0.963\n",
            "Epoch: [140/5000]-----------------------\n",
            "training loss: 0.512\t\t training accuracy: 0.966\n",
            "test loss: 0.509\t\t test accuracy: 0.965\n",
            "Epoch: [150/5000]-----------------------\n",
            "training loss: 0.503\t\t training accuracy: 0.966\n",
            "test loss: 0.500\t\t test accuracy: 0.965\n",
            "Epoch: [160/5000]-----------------------\n",
            "training loss: 0.494\t\t training accuracy: 0.966\n",
            "test loss: 0.491\t\t test accuracy: 0.964\n",
            "Epoch: [170/5000]-----------------------\n",
            "training loss: 0.486\t\t training accuracy: 0.967\n",
            "test loss: 0.482\t\t test accuracy: 0.964\n",
            "Epoch: [180/5000]-----------------------\n",
            "training loss: 0.478\t\t training accuracy: 0.967\n",
            "test loss: 0.474\t\t test accuracy: 0.964\n",
            "Epoch: [190/5000]-----------------------\n",
            "training loss: 0.470\t\t training accuracy: 0.967\n",
            "test loss: 0.466\t\t test accuracy: 0.965\n",
            "Epoch: [200/5000]-----------------------\n",
            "training loss: 0.462\t\t training accuracy: 0.968\n",
            "test loss: 0.458\t\t test accuracy: 0.964\n",
            "Epoch: [210/5000]-----------------------\n",
            "training loss: 0.455\t\t training accuracy: 0.968\n",
            "test loss: 0.451\t\t test accuracy: 0.964\n",
            "Epoch: [220/5000]-----------------------\n",
            "training loss: 0.448\t\t training accuracy: 0.968\n",
            "test loss: 0.444\t\t test accuracy: 0.964\n",
            "Epoch: [230/5000]-----------------------\n",
            "training loss: 0.441\t\t training accuracy: 0.969\n",
            "test loss: 0.437\t\t test accuracy: 0.964\n",
            "Epoch: [240/5000]-----------------------\n",
            "training loss: 0.434\t\t training accuracy: 0.969\n",
            "test loss: 0.431\t\t test accuracy: 0.964\n",
            "Epoch: [250/5000]-----------------------\n",
            "training loss: 0.428\t\t training accuracy: 0.969\n",
            "test loss: 0.424\t\t test accuracy: 0.965\n",
            "Epoch: [260/5000]-----------------------\n",
            "training loss: 0.422\t\t training accuracy: 0.969\n",
            "test loss: 0.418\t\t test accuracy: 0.965\n",
            "Epoch: [270/5000]-----------------------\n",
            "training loss: 0.416\t\t training accuracy: 0.969\n",
            "test loss: 0.412\t\t test accuracy: 0.965\n",
            "Epoch: [280/5000]-----------------------\n",
            "training loss: 0.410\t\t training accuracy: 0.969\n",
            "test loss: 0.406\t\t test accuracy: 0.965\n",
            "Epoch: [290/5000]-----------------------\n",
            "training loss: 0.405\t\t training accuracy: 0.969\n",
            "test loss: 0.401\t\t test accuracy: 0.965\n",
            "Epoch: [300/5000]-----------------------\n",
            "training loss: 0.399\t\t training accuracy: 0.969\n",
            "test loss: 0.395\t\t test accuracy: 0.965\n",
            "Epoch: [310/5000]-----------------------\n",
            "training loss: 0.394\t\t training accuracy: 0.969\n",
            "test loss: 0.390\t\t test accuracy: 0.965\n",
            "Epoch: [320/5000]-----------------------\n",
            "training loss: 0.389\t\t training accuracy: 0.969\n",
            "test loss: 0.385\t\t test accuracy: 0.965\n",
            "Epoch: [330/5000]-----------------------\n",
            "training loss: 0.384\t\t training accuracy: 0.969\n",
            "test loss: 0.380\t\t test accuracy: 0.965\n",
            "Epoch: [340/5000]-----------------------\n",
            "training loss: 0.379\t\t training accuracy: 0.969\n",
            "test loss: 0.375\t\t test accuracy: 0.966\n",
            "Epoch: [350/5000]-----------------------\n",
            "training loss: 0.375\t\t training accuracy: 0.969\n",
            "test loss: 0.371\t\t test accuracy: 0.966\n",
            "Epoch: [360/5000]-----------------------\n",
            "training loss: 0.370\t\t training accuracy: 0.969\n",
            "test loss: 0.366\t\t test accuracy: 0.966\n",
            "Epoch: [370/5000]-----------------------\n",
            "training loss: 0.366\t\t training accuracy: 0.969\n",
            "test loss: 0.362\t\t test accuracy: 0.966\n",
            "Epoch: [380/5000]-----------------------\n",
            "training loss: 0.362\t\t training accuracy: 0.969\n",
            "test loss: 0.358\t\t test accuracy: 0.966\n",
            "Epoch: [390/5000]-----------------------\n",
            "training loss: 0.358\t\t training accuracy: 0.969\n",
            "test loss: 0.354\t\t test accuracy: 0.966\n",
            "Epoch: [400/5000]-----------------------\n",
            "training loss: 0.354\t\t training accuracy: 0.969\n",
            "test loss: 0.350\t\t test accuracy: 0.966\n",
            "Epoch: [410/5000]-----------------------\n",
            "training loss: 0.350\t\t training accuracy: 0.969\n",
            "test loss: 0.346\t\t test accuracy: 0.966\n",
            "Epoch: [420/5000]-----------------------\n",
            "training loss: 0.346\t\t training accuracy: 0.969\n",
            "test loss: 0.342\t\t test accuracy: 0.965\n",
            "Epoch: [430/5000]-----------------------\n",
            "training loss: 0.342\t\t training accuracy: 0.969\n",
            "test loss: 0.338\t\t test accuracy: 0.965\n",
            "Epoch: [440/5000]-----------------------\n",
            "training loss: 0.339\t\t training accuracy: 0.969\n",
            "test loss: 0.335\t\t test accuracy: 0.965\n",
            "Epoch: [450/5000]-----------------------\n",
            "training loss: 0.335\t\t training accuracy: 0.969\n",
            "test loss: 0.331\t\t test accuracy: 0.965\n",
            "Epoch: [460/5000]-----------------------\n",
            "training loss: 0.332\t\t training accuracy: 0.969\n",
            "test loss: 0.328\t\t test accuracy: 0.965\n",
            "Epoch: [470/5000]-----------------------\n",
            "training loss: 0.329\t\t training accuracy: 0.969\n",
            "test loss: 0.325\t\t test accuracy: 0.965\n",
            "Epoch: [480/5000]-----------------------\n",
            "training loss: 0.325\t\t training accuracy: 0.969\n",
            "test loss: 0.322\t\t test accuracy: 0.965\n",
            "Epoch: [490/5000]-----------------------\n",
            "training loss: 0.322\t\t training accuracy: 0.969\n",
            "test loss: 0.318\t\t test accuracy: 0.965\n",
            "Epoch: [500/5000]-----------------------\n",
            "training loss: 0.319\t\t training accuracy: 0.969\n",
            "test loss: 0.315\t\t test accuracy: 0.965\n",
            "Epoch: [510/5000]-----------------------\n",
            "training loss: 0.316\t\t training accuracy: 0.969\n",
            "test loss: 0.312\t\t test accuracy: 0.965\n",
            "Epoch: [520/5000]-----------------------\n",
            "training loss: 0.313\t\t training accuracy: 0.969\n",
            "test loss: 0.310\t\t test accuracy: 0.965\n",
            "Epoch: [530/5000]-----------------------\n",
            "training loss: 0.311\t\t training accuracy: 0.969\n",
            "test loss: 0.307\t\t test accuracy: 0.965\n",
            "Epoch: [540/5000]-----------------------\n",
            "training loss: 0.308\t\t training accuracy: 0.969\n",
            "test loss: 0.304\t\t test accuracy: 0.965\n",
            "Epoch: [550/5000]-----------------------\n",
            "training loss: 0.305\t\t training accuracy: 0.969\n",
            "test loss: 0.301\t\t test accuracy: 0.965\n",
            "Epoch: [560/5000]-----------------------\n",
            "training loss: 0.303\t\t training accuracy: 0.969\n",
            "test loss: 0.299\t\t test accuracy: 0.965\n",
            "Epoch: [570/5000]-----------------------\n",
            "training loss: 0.300\t\t training accuracy: 0.969\n",
            "test loss: 0.296\t\t test accuracy: 0.965\n",
            "Epoch: [580/5000]-----------------------\n",
            "training loss: 0.298\t\t training accuracy: 0.969\n",
            "test loss: 0.294\t\t test accuracy: 0.965\n",
            "Epoch: [590/5000]-----------------------\n",
            "training loss: 0.295\t\t training accuracy: 0.969\n",
            "test loss: 0.291\t\t test accuracy: 0.965\n",
            "Epoch: [600/5000]-----------------------\n",
            "training loss: 0.293\t\t training accuracy: 0.969\n",
            "test loss: 0.289\t\t test accuracy: 0.965\n",
            "Epoch: [610/5000]-----------------------\n",
            "training loss: 0.290\t\t training accuracy: 0.969\n",
            "test loss: 0.287\t\t test accuracy: 0.965\n",
            "Epoch: [620/5000]-----------------------\n",
            "training loss: 0.288\t\t training accuracy: 0.969\n",
            "test loss: 0.284\t\t test accuracy: 0.965\n",
            "Epoch: [630/5000]-----------------------\n",
            "training loss: 0.286\t\t training accuracy: 0.969\n",
            "test loss: 0.282\t\t test accuracy: 0.965\n",
            "Epoch: [640/5000]-----------------------\n",
            "training loss: 0.284\t\t training accuracy: 0.969\n",
            "test loss: 0.280\t\t test accuracy: 0.965\n",
            "Epoch: [650/5000]-----------------------\n",
            "training loss: 0.282\t\t training accuracy: 0.969\n",
            "test loss: 0.278\t\t test accuracy: 0.965\n",
            "Epoch: [660/5000]-----------------------\n",
            "training loss: 0.279\t\t training accuracy: 0.969\n",
            "test loss: 0.276\t\t test accuracy: 0.965\n",
            "Epoch: [670/5000]-----------------------\n",
            "training loss: 0.277\t\t training accuracy: 0.969\n",
            "test loss: 0.274\t\t test accuracy: 0.965\n",
            "Epoch: [680/5000]-----------------------\n",
            "training loss: 0.275\t\t training accuracy: 0.969\n",
            "test loss: 0.272\t\t test accuracy: 0.965\n",
            "Epoch: [690/5000]-----------------------\n",
            "training loss: 0.273\t\t training accuracy: 0.969\n",
            "test loss: 0.270\t\t test accuracy: 0.965\n",
            "Epoch: [700/5000]-----------------------\n",
            "training loss: 0.272\t\t training accuracy: 0.969\n",
            "test loss: 0.268\t\t test accuracy: 0.965\n",
            "Epoch: [710/5000]-----------------------\n",
            "training loss: 0.270\t\t training accuracy: 0.969\n",
            "test loss: 0.266\t\t test accuracy: 0.965\n",
            "Epoch: [720/5000]-----------------------\n",
            "training loss: 0.268\t\t training accuracy: 0.969\n",
            "test loss: 0.264\t\t test accuracy: 0.965\n",
            "Epoch: [730/5000]-----------------------\n",
            "training loss: 0.266\t\t training accuracy: 0.969\n",
            "test loss: 0.262\t\t test accuracy: 0.965\n",
            "Epoch: [740/5000]-----------------------\n",
            "training loss: 0.264\t\t training accuracy: 0.969\n",
            "test loss: 0.261\t\t test accuracy: 0.965\n",
            "Epoch: [750/5000]-----------------------\n",
            "training loss: 0.263\t\t training accuracy: 0.969\n",
            "test loss: 0.259\t\t test accuracy: 0.965\n",
            "Epoch: [760/5000]-----------------------\n",
            "training loss: 0.261\t\t training accuracy: 0.970\n",
            "test loss: 0.257\t\t test accuracy: 0.965\n",
            "Epoch: [770/5000]-----------------------\n",
            "training loss: 0.259\t\t training accuracy: 0.970\n",
            "test loss: 0.256\t\t test accuracy: 0.965\n",
            "Epoch: [780/5000]-----------------------\n",
            "training loss: 0.258\t\t training accuracy: 0.970\n",
            "test loss: 0.254\t\t test accuracy: 0.965\n",
            "Epoch: [790/5000]-----------------------\n",
            "training loss: 0.256\t\t training accuracy: 0.970\n",
            "test loss: 0.252\t\t test accuracy: 0.965\n",
            "Epoch: [800/5000]-----------------------\n",
            "training loss: 0.254\t\t training accuracy: 0.970\n",
            "test loss: 0.251\t\t test accuracy: 0.965\n",
            "Epoch: [810/5000]-----------------------\n",
            "training loss: 0.253\t\t training accuracy: 0.970\n",
            "test loss: 0.249\t\t test accuracy: 0.965\n",
            "Epoch: [820/5000]-----------------------\n",
            "training loss: 0.251\t\t training accuracy: 0.970\n",
            "test loss: 0.248\t\t test accuracy: 0.965\n",
            "Epoch: [830/5000]-----------------------\n",
            "training loss: 0.250\t\t training accuracy: 0.970\n",
            "test loss: 0.246\t\t test accuracy: 0.965\n",
            "Epoch: [840/5000]-----------------------\n",
            "training loss: 0.248\t\t training accuracy: 0.970\n",
            "test loss: 0.245\t\t test accuracy: 0.965\n",
            "Epoch: [850/5000]-----------------------\n",
            "training loss: 0.247\t\t training accuracy: 0.970\n",
            "test loss: 0.243\t\t test accuracy: 0.965\n",
            "Epoch: [860/5000]-----------------------\n",
            "training loss: 0.245\t\t training accuracy: 0.970\n",
            "test loss: 0.242\t\t test accuracy: 0.965\n",
            "Epoch: [870/5000]-----------------------\n",
            "training loss: 0.244\t\t training accuracy: 0.970\n",
            "test loss: 0.241\t\t test accuracy: 0.965\n",
            "Epoch: [880/5000]-----------------------\n",
            "training loss: 0.243\t\t training accuracy: 0.970\n",
            "test loss: 0.239\t\t test accuracy: 0.965\n",
            "Epoch: [890/5000]-----------------------\n",
            "training loss: 0.241\t\t training accuracy: 0.970\n",
            "test loss: 0.238\t\t test accuracy: 0.965\n",
            "Epoch: [900/5000]-----------------------\n",
            "training loss: 0.240\t\t training accuracy: 0.970\n",
            "test loss: 0.237\t\t test accuracy: 0.965\n",
            "Epoch: [910/5000]-----------------------\n",
            "training loss: 0.239\t\t training accuracy: 0.970\n",
            "test loss: 0.235\t\t test accuracy: 0.965\n",
            "Epoch: [920/5000]-----------------------\n",
            "training loss: 0.238\t\t training accuracy: 0.970\n",
            "test loss: 0.234\t\t test accuracy: 0.965\n",
            "Epoch: [930/5000]-----------------------\n",
            "training loss: 0.236\t\t training accuracy: 0.970\n",
            "test loss: 0.233\t\t test accuracy: 0.965\n",
            "Epoch: [940/5000]-----------------------\n",
            "training loss: 0.235\t\t training accuracy: 0.970\n",
            "test loss: 0.232\t\t test accuracy: 0.965\n",
            "Epoch: [950/5000]-----------------------\n",
            "training loss: 0.234\t\t training accuracy: 0.970\n",
            "test loss: 0.230\t\t test accuracy: 0.965\n",
            "Epoch: [960/5000]-----------------------\n",
            "training loss: 0.233\t\t training accuracy: 0.970\n",
            "test loss: 0.229\t\t test accuracy: 0.965\n",
            "Epoch: [970/5000]-----------------------\n",
            "training loss: 0.231\t\t training accuracy: 0.970\n",
            "test loss: 0.228\t\t test accuracy: 0.965\n",
            "Epoch: [980/5000]-----------------------\n",
            "training loss: 0.230\t\t training accuracy: 0.970\n",
            "test loss: 0.227\t\t test accuracy: 0.965\n",
            "Epoch: [990/5000]-----------------------\n",
            "training loss: 0.229\t\t training accuracy: 0.970\n",
            "test loss: 0.226\t\t test accuracy: 0.965\n",
            "Epoch: [1000/5000]-----------------------\n",
            "training loss: 0.228\t\t training accuracy: 0.970\n",
            "test loss: 0.225\t\t test accuracy: 0.965\n",
            "Epoch: [1010/5000]-----------------------\n",
            "training loss: 0.227\t\t training accuracy: 0.970\n",
            "test loss: 0.223\t\t test accuracy: 0.966\n",
            "Epoch: [1020/5000]-----------------------\n",
            "training loss: 0.226\t\t training accuracy: 0.970\n",
            "test loss: 0.222\t\t test accuracy: 0.966\n",
            "Epoch: [1030/5000]-----------------------\n",
            "training loss: 0.225\t\t training accuracy: 0.970\n",
            "test loss: 0.221\t\t test accuracy: 0.966\n",
            "Epoch: [1040/5000]-----------------------\n",
            "training loss: 0.224\t\t training accuracy: 0.970\n",
            "test loss: 0.220\t\t test accuracy: 0.966\n",
            "Epoch: [1050/5000]-----------------------\n",
            "training loss: 0.223\t\t training accuracy: 0.970\n",
            "test loss: 0.219\t\t test accuracy: 0.966\n",
            "Epoch: [1060/5000]-----------------------\n",
            "training loss: 0.222\t\t training accuracy: 0.970\n",
            "test loss: 0.218\t\t test accuracy: 0.966\n",
            "Epoch: [1070/5000]-----------------------\n",
            "training loss: 0.221\t\t training accuracy: 0.970\n",
            "test loss: 0.217\t\t test accuracy: 0.966\n",
            "Epoch: [1080/5000]-----------------------\n",
            "training loss: 0.220\t\t training accuracy: 0.970\n",
            "test loss: 0.216\t\t test accuracy: 0.966\n",
            "Epoch: [1090/5000]-----------------------\n",
            "training loss: 0.219\t\t training accuracy: 0.970\n",
            "test loss: 0.215\t\t test accuracy: 0.966\n",
            "Epoch: [1100/5000]-----------------------\n",
            "training loss: 0.218\t\t training accuracy: 0.970\n",
            "test loss: 0.214\t\t test accuracy: 0.966\n",
            "Epoch: [1110/5000]-----------------------\n",
            "training loss: 0.217\t\t training accuracy: 0.970\n",
            "test loss: 0.213\t\t test accuracy: 0.966\n",
            "Epoch: [1120/5000]-----------------------\n",
            "training loss: 0.216\t\t training accuracy: 0.970\n",
            "test loss: 0.213\t\t test accuracy: 0.966\n",
            "Epoch: [1130/5000]-----------------------\n",
            "training loss: 0.215\t\t training accuracy: 0.970\n",
            "test loss: 0.212\t\t test accuracy: 0.966\n",
            "Epoch: [1140/5000]-----------------------\n",
            "training loss: 0.214\t\t training accuracy: 0.970\n",
            "test loss: 0.211\t\t test accuracy: 0.966\n",
            "Epoch: [1150/5000]-----------------------\n",
            "training loss: 0.213\t\t training accuracy: 0.970\n",
            "test loss: 0.210\t\t test accuracy: 0.966\n",
            "Epoch: [1160/5000]-----------------------\n",
            "training loss: 0.212\t\t training accuracy: 0.970\n",
            "test loss: 0.209\t\t test accuracy: 0.966\n",
            "Epoch: [1170/5000]-----------------------\n",
            "training loss: 0.211\t\t training accuracy: 0.970\n",
            "test loss: 0.208\t\t test accuracy: 0.966\n",
            "Epoch: [1180/5000]-----------------------\n",
            "training loss: 0.211\t\t training accuracy: 0.970\n",
            "test loss: 0.207\t\t test accuracy: 0.966\n",
            "Epoch: [1190/5000]-----------------------\n",
            "training loss: 0.210\t\t training accuracy: 0.970\n",
            "test loss: 0.206\t\t test accuracy: 0.966\n",
            "Epoch: [1200/5000]-----------------------\n",
            "training loss: 0.209\t\t training accuracy: 0.970\n",
            "test loss: 0.206\t\t test accuracy: 0.966\n",
            "Epoch: [1210/5000]-----------------------\n",
            "training loss: 0.208\t\t training accuracy: 0.970\n",
            "test loss: 0.205\t\t test accuracy: 0.966\n",
            "Epoch: [1220/5000]-----------------------\n",
            "training loss: 0.207\t\t training accuracy: 0.970\n",
            "test loss: 0.204\t\t test accuracy: 0.966\n",
            "Epoch: [1230/5000]-----------------------\n",
            "training loss: 0.206\t\t training accuracy: 0.970\n",
            "test loss: 0.203\t\t test accuracy: 0.966\n",
            "Epoch: [1240/5000]-----------------------\n",
            "training loss: 0.206\t\t training accuracy: 0.970\n",
            "test loss: 0.202\t\t test accuracy: 0.966\n",
            "Epoch: [1250/5000]-----------------------\n",
            "training loss: 0.205\t\t training accuracy: 0.970\n",
            "test loss: 0.202\t\t test accuracy: 0.966\n",
            "Epoch: [1260/5000]-----------------------\n",
            "training loss: 0.204\t\t training accuracy: 0.970\n",
            "test loss: 0.201\t\t test accuracy: 0.966\n",
            "Epoch: [1270/5000]-----------------------\n",
            "training loss: 0.203\t\t training accuracy: 0.970\n",
            "test loss: 0.200\t\t test accuracy: 0.966\n",
            "Epoch: [1280/5000]-----------------------\n",
            "training loss: 0.203\t\t training accuracy: 0.970\n",
            "test loss: 0.199\t\t test accuracy: 0.966\n",
            "Epoch: [1290/5000]-----------------------\n",
            "training loss: 0.202\t\t training accuracy: 0.970\n",
            "test loss: 0.199\t\t test accuracy: 0.966\n",
            "Epoch: [1300/5000]-----------------------\n",
            "training loss: 0.201\t\t training accuracy: 0.970\n",
            "test loss: 0.198\t\t test accuracy: 0.966\n",
            "Epoch: [1310/5000]-----------------------\n",
            "training loss: 0.200\t\t training accuracy: 0.970\n",
            "test loss: 0.197\t\t test accuracy: 0.966\n",
            "Epoch: [1320/5000]-----------------------\n",
            "training loss: 0.200\t\t training accuracy: 0.970\n",
            "test loss: 0.196\t\t test accuracy: 0.966\n",
            "Epoch: [1330/5000]-----------------------\n",
            "training loss: 0.199\t\t training accuracy: 0.970\n",
            "test loss: 0.196\t\t test accuracy: 0.966\n",
            "Epoch: [1340/5000]-----------------------\n",
            "training loss: 0.198\t\t training accuracy: 0.970\n",
            "test loss: 0.195\t\t test accuracy: 0.966\n",
            "Epoch: [1350/5000]-----------------------\n",
            "training loss: 0.198\t\t training accuracy: 0.970\n",
            "test loss: 0.194\t\t test accuracy: 0.966\n",
            "Epoch: [1360/5000]-----------------------\n",
            "training loss: 0.197\t\t training accuracy: 0.970\n",
            "test loss: 0.194\t\t test accuracy: 0.966\n",
            "Epoch: [1370/5000]-----------------------\n",
            "training loss: 0.196\t\t training accuracy: 0.970\n",
            "test loss: 0.193\t\t test accuracy: 0.966\n",
            "Epoch: [1380/5000]-----------------------\n",
            "training loss: 0.196\t\t training accuracy: 0.971\n",
            "test loss: 0.192\t\t test accuracy: 0.966\n",
            "Epoch: [1390/5000]-----------------------\n",
            "training loss: 0.195\t\t training accuracy: 0.971\n",
            "test loss: 0.192\t\t test accuracy: 0.966\n",
            "Epoch: [1400/5000]-----------------------\n",
            "training loss: 0.194\t\t training accuracy: 0.971\n",
            "test loss: 0.191\t\t test accuracy: 0.966\n",
            "Epoch: [1410/5000]-----------------------\n",
            "training loss: 0.194\t\t training accuracy: 0.971\n",
            "test loss: 0.191\t\t test accuracy: 0.966\n",
            "Epoch: [1420/5000]-----------------------\n",
            "training loss: 0.193\t\t training accuracy: 0.971\n",
            "test loss: 0.190\t\t test accuracy: 0.966\n",
            "Epoch: [1430/5000]-----------------------\n",
            "training loss: 0.192\t\t training accuracy: 0.971\n",
            "test loss: 0.189\t\t test accuracy: 0.966\n",
            "Epoch: [1440/5000]-----------------------\n",
            "training loss: 0.192\t\t training accuracy: 0.971\n",
            "test loss: 0.189\t\t test accuracy: 0.966\n",
            "Epoch: [1450/5000]-----------------------\n",
            "training loss: 0.191\t\t training accuracy: 0.971\n",
            "test loss: 0.188\t\t test accuracy: 0.966\n",
            "Epoch: [1460/5000]-----------------------\n",
            "training loss: 0.191\t\t training accuracy: 0.971\n",
            "test loss: 0.187\t\t test accuracy: 0.966\n",
            "Epoch: [1470/5000]-----------------------\n",
            "training loss: 0.190\t\t training accuracy: 0.971\n",
            "test loss: 0.187\t\t test accuracy: 0.966\n",
            "Epoch: [1480/5000]-----------------------\n",
            "training loss: 0.189\t\t training accuracy: 0.971\n",
            "test loss: 0.186\t\t test accuracy: 0.966\n",
            "Epoch: [1490/5000]-----------------------\n",
            "training loss: 0.189\t\t training accuracy: 0.971\n",
            "test loss: 0.186\t\t test accuracy: 0.966\n",
            "Epoch: [1500/5000]-----------------------\n",
            "training loss: 0.188\t\t training accuracy: 0.971\n",
            "test loss: 0.185\t\t test accuracy: 0.966\n",
            "Epoch: [1510/5000]-----------------------\n",
            "training loss: 0.188\t\t training accuracy: 0.971\n",
            "test loss: 0.185\t\t test accuracy: 0.966\n",
            "Epoch: [1520/5000]-----------------------\n",
            "training loss: 0.187\t\t training accuracy: 0.971\n",
            "test loss: 0.184\t\t test accuracy: 0.966\n",
            "Epoch: [1530/5000]-----------------------\n",
            "training loss: 0.187\t\t training accuracy: 0.971\n",
            "test loss: 0.183\t\t test accuracy: 0.966\n",
            "Epoch: [1540/5000]-----------------------\n",
            "training loss: 0.186\t\t training accuracy: 0.971\n",
            "test loss: 0.183\t\t test accuracy: 0.966\n",
            "Epoch: [1550/5000]-----------------------\n",
            "training loss: 0.186\t\t training accuracy: 0.971\n",
            "test loss: 0.182\t\t test accuracy: 0.966\n",
            "Epoch: [1560/5000]-----------------------\n",
            "training loss: 0.185\t\t training accuracy: 0.971\n",
            "test loss: 0.182\t\t test accuracy: 0.966\n",
            "Epoch: [1570/5000]-----------------------\n",
            "training loss: 0.184\t\t training accuracy: 0.971\n",
            "test loss: 0.181\t\t test accuracy: 0.966\n",
            "Epoch: [1580/5000]-----------------------\n",
            "training loss: 0.184\t\t training accuracy: 0.971\n",
            "test loss: 0.181\t\t test accuracy: 0.966\n",
            "Epoch: [1590/5000]-----------------------\n",
            "training loss: 0.183\t\t training accuracy: 0.971\n",
            "test loss: 0.180\t\t test accuracy: 0.966\n",
            "Epoch: [1600/5000]-----------------------\n",
            "training loss: 0.183\t\t training accuracy: 0.971\n",
            "test loss: 0.180\t\t test accuracy: 0.966\n",
            "Epoch: [1610/5000]-----------------------\n",
            "training loss: 0.182\t\t training accuracy: 0.971\n",
            "test loss: 0.179\t\t test accuracy: 0.966\n",
            "Epoch: [1620/5000]-----------------------\n",
            "training loss: 0.182\t\t training accuracy: 0.971\n",
            "test loss: 0.179\t\t test accuracy: 0.966\n",
            "Epoch: [1630/5000]-----------------------\n",
            "training loss: 0.181\t\t training accuracy: 0.971\n",
            "test loss: 0.178\t\t test accuracy: 0.966\n",
            "Epoch: [1640/5000]-----------------------\n",
            "training loss: 0.181\t\t training accuracy: 0.971\n",
            "test loss: 0.178\t\t test accuracy: 0.966\n",
            "Epoch: [1650/5000]-----------------------\n",
            "training loss: 0.180\t\t training accuracy: 0.971\n",
            "test loss: 0.177\t\t test accuracy: 0.966\n",
            "Epoch: [1660/5000]-----------------------\n",
            "training loss: 0.180\t\t training accuracy: 0.971\n",
            "test loss: 0.177\t\t test accuracy: 0.966\n",
            "Epoch: [1670/5000]-----------------------\n",
            "training loss: 0.179\t\t training accuracy: 0.971\n",
            "test loss: 0.176\t\t test accuracy: 0.966\n",
            "Epoch: [1680/5000]-----------------------\n",
            "training loss: 0.179\t\t training accuracy: 0.971\n",
            "test loss: 0.176\t\t test accuracy: 0.966\n",
            "Epoch: [1690/5000]-----------------------\n",
            "training loss: 0.179\t\t training accuracy: 0.971\n",
            "test loss: 0.175\t\t test accuracy: 0.966\n",
            "Epoch: [1700/5000]-----------------------\n",
            "training loss: 0.178\t\t training accuracy: 0.971\n",
            "test loss: 0.175\t\t test accuracy: 0.966\n",
            "Epoch: [1710/5000]-----------------------\n",
            "training loss: 0.178\t\t training accuracy: 0.971\n",
            "test loss: 0.175\t\t test accuracy: 0.966\n",
            "Epoch: [1720/5000]-----------------------\n",
            "training loss: 0.177\t\t training accuracy: 0.971\n",
            "test loss: 0.174\t\t test accuracy: 0.966\n",
            "Epoch: [1730/5000]-----------------------\n",
            "training loss: 0.177\t\t training accuracy: 0.971\n",
            "test loss: 0.174\t\t test accuracy: 0.966\n",
            "Epoch: [1740/5000]-----------------------\n",
            "training loss: 0.176\t\t training accuracy: 0.971\n",
            "test loss: 0.173\t\t test accuracy: 0.966\n",
            "Epoch: [1750/5000]-----------------------\n",
            "training loss: 0.176\t\t training accuracy: 0.971\n",
            "test loss: 0.173\t\t test accuracy: 0.966\n",
            "Epoch: [1760/5000]-----------------------\n",
            "training loss: 0.175\t\t training accuracy: 0.971\n",
            "test loss: 0.172\t\t test accuracy: 0.966\n",
            "Epoch: [1770/5000]-----------------------\n",
            "training loss: 0.175\t\t training accuracy: 0.971\n",
            "test loss: 0.172\t\t test accuracy: 0.966\n",
            "Epoch: [1780/5000]-----------------------\n",
            "training loss: 0.175\t\t training accuracy: 0.971\n",
            "test loss: 0.171\t\t test accuracy: 0.966\n",
            "Epoch: [1790/5000]-----------------------\n",
            "training loss: 0.174\t\t training accuracy: 0.971\n",
            "test loss: 0.171\t\t test accuracy: 0.966\n",
            "Epoch: [1800/5000]-----------------------\n",
            "training loss: 0.174\t\t training accuracy: 0.971\n",
            "test loss: 0.171\t\t test accuracy: 0.966\n",
            "Epoch: [1810/5000]-----------------------\n",
            "training loss: 0.173\t\t training accuracy: 0.971\n",
            "test loss: 0.170\t\t test accuracy: 0.966\n",
            "Epoch: [1820/5000]-----------------------\n",
            "training loss: 0.173\t\t training accuracy: 0.971\n",
            "test loss: 0.170\t\t test accuracy: 0.966\n",
            "Epoch: [1830/5000]-----------------------\n",
            "training loss: 0.172\t\t training accuracy: 0.971\n",
            "test loss: 0.169\t\t test accuracy: 0.966\n",
            "Epoch: [1840/5000]-----------------------\n",
            "training loss: 0.172\t\t training accuracy: 0.971\n",
            "test loss: 0.169\t\t test accuracy: 0.966\n",
            "Epoch: [1850/5000]-----------------------\n",
            "training loss: 0.172\t\t training accuracy: 0.971\n",
            "test loss: 0.169\t\t test accuracy: 0.966\n",
            "Epoch: [1860/5000]-----------------------\n",
            "training loss: 0.171\t\t training accuracy: 0.971\n",
            "test loss: 0.168\t\t test accuracy: 0.966\n",
            "Epoch: [1870/5000]-----------------------\n",
            "training loss: 0.171\t\t training accuracy: 0.971\n",
            "test loss: 0.168\t\t test accuracy: 0.966\n",
            "Epoch: [1880/5000]-----------------------\n",
            "training loss: 0.170\t\t training accuracy: 0.971\n",
            "test loss: 0.167\t\t test accuracy: 0.966\n",
            "Epoch: [1890/5000]-----------------------\n",
            "training loss: 0.170\t\t training accuracy: 0.971\n",
            "test loss: 0.167\t\t test accuracy: 0.966\n",
            "Epoch: [1900/5000]-----------------------\n",
            "training loss: 0.170\t\t training accuracy: 0.971\n",
            "test loss: 0.167\t\t test accuracy: 0.966\n",
            "Epoch: [1910/5000]-----------------------\n",
            "training loss: 0.169\t\t training accuracy: 0.971\n",
            "test loss: 0.166\t\t test accuracy: 0.966\n",
            "Epoch: [1920/5000]-----------------------\n",
            "training loss: 0.169\t\t training accuracy: 0.971\n",
            "test loss: 0.166\t\t test accuracy: 0.966\n",
            "Epoch: [1930/5000]-----------------------\n",
            "training loss: 0.169\t\t training accuracy: 0.971\n",
            "test loss: 0.166\t\t test accuracy: 0.966\n",
            "Epoch: [1940/5000]-----------------------\n",
            "training loss: 0.168\t\t training accuracy: 0.971\n",
            "test loss: 0.165\t\t test accuracy: 0.966\n",
            "Epoch: [1950/5000]-----------------------\n",
            "training loss: 0.168\t\t training accuracy: 0.971\n",
            "test loss: 0.165\t\t test accuracy: 0.966\n",
            "Epoch: [1960/5000]-----------------------\n",
            "training loss: 0.167\t\t training accuracy: 0.971\n",
            "test loss: 0.164\t\t test accuracy: 0.967\n",
            "Epoch: [1970/5000]-----------------------\n",
            "training loss: 0.167\t\t training accuracy: 0.971\n",
            "test loss: 0.164\t\t test accuracy: 0.967\n",
            "Epoch: [1980/5000]-----------------------\n",
            "training loss: 0.167\t\t training accuracy: 0.971\n",
            "test loss: 0.164\t\t test accuracy: 0.967\n",
            "Epoch: [1990/5000]-----------------------\n",
            "training loss: 0.166\t\t training accuracy: 0.971\n",
            "test loss: 0.163\t\t test accuracy: 0.967\n",
            "Epoch: [2000/5000]-----------------------\n",
            "training loss: 0.166\t\t training accuracy: 0.971\n",
            "test loss: 0.163\t\t test accuracy: 0.967\n",
            "Epoch: [2010/5000]-----------------------\n",
            "training loss: 0.166\t\t training accuracy: 0.971\n",
            "test loss: 0.163\t\t test accuracy: 0.967\n",
            "Epoch: [2020/5000]-----------------------\n",
            "training loss: 0.165\t\t training accuracy: 0.971\n",
            "test loss: 0.162\t\t test accuracy: 0.967\n",
            "Epoch: [2030/5000]-----------------------\n",
            "training loss: 0.165\t\t training accuracy: 0.971\n",
            "test loss: 0.162\t\t test accuracy: 0.967\n",
            "Epoch: [2040/5000]-----------------------\n",
            "training loss: 0.165\t\t training accuracy: 0.971\n",
            "test loss: 0.162\t\t test accuracy: 0.967\n",
            "Epoch: [2050/5000]-----------------------\n",
            "training loss: 0.164\t\t training accuracy: 0.971\n",
            "test loss: 0.161\t\t test accuracy: 0.967\n",
            "Epoch: [2060/5000]-----------------------\n",
            "training loss: 0.164\t\t training accuracy: 0.971\n",
            "test loss: 0.161\t\t test accuracy: 0.967\n",
            "Epoch: [2070/5000]-----------------------\n",
            "training loss: 0.164\t\t training accuracy: 0.971\n",
            "test loss: 0.161\t\t test accuracy: 0.967\n",
            "Epoch: [2080/5000]-----------------------\n",
            "training loss: 0.163\t\t training accuracy: 0.971\n",
            "test loss: 0.160\t\t test accuracy: 0.967\n",
            "Epoch: [2090/5000]-----------------------\n",
            "training loss: 0.163\t\t training accuracy: 0.972\n",
            "test loss: 0.160\t\t test accuracy: 0.967\n",
            "Epoch: [2100/5000]-----------------------\n",
            "training loss: 0.163\t\t training accuracy: 0.972\n",
            "test loss: 0.160\t\t test accuracy: 0.967\n",
            "Epoch: [2110/5000]-----------------------\n",
            "training loss: 0.162\t\t training accuracy: 0.972\n",
            "test loss: 0.159\t\t test accuracy: 0.967\n",
            "Epoch: [2120/5000]-----------------------\n",
            "training loss: 0.162\t\t training accuracy: 0.972\n",
            "test loss: 0.159\t\t test accuracy: 0.967\n",
            "Epoch: [2130/5000]-----------------------\n",
            "training loss: 0.162\t\t training accuracy: 0.972\n",
            "test loss: 0.159\t\t test accuracy: 0.967\n",
            "Epoch: [2140/5000]-----------------------\n",
            "training loss: 0.161\t\t training accuracy: 0.972\n",
            "test loss: 0.158\t\t test accuracy: 0.967\n",
            "Epoch: [2150/5000]-----------------------\n",
            "training loss: 0.161\t\t training accuracy: 0.972\n",
            "test loss: 0.158\t\t test accuracy: 0.967\n",
            "Epoch: [2160/5000]-----------------------\n",
            "training loss: 0.161\t\t training accuracy: 0.972\n",
            "test loss: 0.158\t\t test accuracy: 0.967\n",
            "Epoch: [2170/5000]-----------------------\n",
            "training loss: 0.160\t\t training accuracy: 0.972\n",
            "test loss: 0.158\t\t test accuracy: 0.967\n",
            "Epoch: [2180/5000]-----------------------\n",
            "training loss: 0.160\t\t training accuracy: 0.972\n",
            "test loss: 0.157\t\t test accuracy: 0.967\n",
            "Epoch: [2190/5000]-----------------------\n",
            "training loss: 0.160\t\t training accuracy: 0.972\n",
            "test loss: 0.157\t\t test accuracy: 0.967\n",
            "Epoch: [2200/5000]-----------------------\n",
            "training loss: 0.160\t\t training accuracy: 0.972\n",
            "test loss: 0.157\t\t test accuracy: 0.967\n",
            "Epoch: [2210/5000]-----------------------\n",
            "training loss: 0.159\t\t training accuracy: 0.972\n",
            "test loss: 0.156\t\t test accuracy: 0.967\n",
            "Epoch: [2220/5000]-----------------------\n",
            "training loss: 0.159\t\t training accuracy: 0.972\n",
            "test loss: 0.156\t\t test accuracy: 0.967\n",
            "Epoch: [2230/5000]-----------------------\n",
            "training loss: 0.159\t\t training accuracy: 0.972\n",
            "test loss: 0.156\t\t test accuracy: 0.967\n",
            "Epoch: [2240/5000]-----------------------\n",
            "training loss: 0.158\t\t training accuracy: 0.972\n",
            "test loss: 0.156\t\t test accuracy: 0.967\n",
            "Epoch: [2250/5000]-----------------------\n",
            "training loss: 0.158\t\t training accuracy: 0.972\n",
            "test loss: 0.155\t\t test accuracy: 0.967\n",
            "Epoch: [2260/5000]-----------------------\n",
            "training loss: 0.158\t\t training accuracy: 0.972\n",
            "test loss: 0.155\t\t test accuracy: 0.967\n",
            "Epoch: [2270/5000]-----------------------\n",
            "training loss: 0.158\t\t training accuracy: 0.972\n",
            "test loss: 0.155\t\t test accuracy: 0.967\n",
            "Epoch: [2280/5000]-----------------------\n",
            "training loss: 0.157\t\t training accuracy: 0.972\n",
            "test loss: 0.154\t\t test accuracy: 0.967\n",
            "Epoch: [2290/5000]-----------------------\n",
            "training loss: 0.157\t\t training accuracy: 0.972\n",
            "test loss: 0.154\t\t test accuracy: 0.967\n",
            "Epoch: [2300/5000]-----------------------\n",
            "training loss: 0.157\t\t training accuracy: 0.972\n",
            "test loss: 0.154\t\t test accuracy: 0.967\n",
            "Epoch: [2310/5000]-----------------------\n",
            "training loss: 0.156\t\t training accuracy: 0.972\n",
            "test loss: 0.154\t\t test accuracy: 0.967\n",
            "Epoch: [2320/5000]-----------------------\n",
            "training loss: 0.156\t\t training accuracy: 0.972\n",
            "test loss: 0.153\t\t test accuracy: 0.967\n",
            "Epoch: [2330/5000]-----------------------\n",
            "training loss: 0.156\t\t training accuracy: 0.972\n",
            "test loss: 0.153\t\t test accuracy: 0.967\n",
            "Epoch: [2340/5000]-----------------------\n",
            "training loss: 0.156\t\t training accuracy: 0.972\n",
            "test loss: 0.153\t\t test accuracy: 0.967\n",
            "Epoch: [2350/5000]-----------------------\n",
            "training loss: 0.155\t\t training accuracy: 0.972\n",
            "test loss: 0.153\t\t test accuracy: 0.967\n",
            "Epoch: [2360/5000]-----------------------\n",
            "training loss: 0.155\t\t training accuracy: 0.972\n",
            "test loss: 0.152\t\t test accuracy: 0.967\n",
            "Epoch: [2370/5000]-----------------------\n",
            "training loss: 0.155\t\t training accuracy: 0.972\n",
            "test loss: 0.152\t\t test accuracy: 0.967\n",
            "Epoch: [2380/5000]-----------------------\n",
            "training loss: 0.155\t\t training accuracy: 0.972\n",
            "test loss: 0.152\t\t test accuracy: 0.967\n",
            "Epoch: [2390/5000]-----------------------\n",
            "training loss: 0.154\t\t training accuracy: 0.972\n",
            "test loss: 0.151\t\t test accuracy: 0.967\n",
            "Epoch: [2400/5000]-----------------------\n",
            "training loss: 0.154\t\t training accuracy: 0.972\n",
            "test loss: 0.151\t\t test accuracy: 0.967\n",
            "Epoch: [2410/5000]-----------------------\n",
            "training loss: 0.154\t\t training accuracy: 0.972\n",
            "test loss: 0.151\t\t test accuracy: 0.967\n",
            "Epoch: [2420/5000]-----------------------\n",
            "training loss: 0.154\t\t training accuracy: 0.972\n",
            "test loss: 0.151\t\t test accuracy: 0.967\n",
            "Epoch: [2430/5000]-----------------------\n",
            "training loss: 0.153\t\t training accuracy: 0.972\n",
            "test loss: 0.150\t\t test accuracy: 0.967\n",
            "Epoch: [2440/5000]-----------------------\n",
            "training loss: 0.153\t\t training accuracy: 0.972\n",
            "test loss: 0.150\t\t test accuracy: 0.967\n",
            "Epoch: [2450/5000]-----------------------\n",
            "training loss: 0.153\t\t training accuracy: 0.972\n",
            "test loss: 0.150\t\t test accuracy: 0.967\n",
            "Epoch: [2460/5000]-----------------------\n",
            "training loss: 0.153\t\t training accuracy: 0.972\n",
            "test loss: 0.150\t\t test accuracy: 0.967\n",
            "Epoch: [2470/5000]-----------------------\n",
            "training loss: 0.152\t\t training accuracy: 0.972\n",
            "test loss: 0.149\t\t test accuracy: 0.967\n",
            "Epoch: [2480/5000]-----------------------\n",
            "training loss: 0.152\t\t training accuracy: 0.972\n",
            "test loss: 0.149\t\t test accuracy: 0.967\n",
            "Epoch: [2490/5000]-----------------------\n",
            "training loss: 0.152\t\t training accuracy: 0.972\n",
            "test loss: 0.149\t\t test accuracy: 0.967\n",
            "Epoch: [2500/5000]-----------------------\n",
            "training loss: 0.152\t\t training accuracy: 0.972\n",
            "test loss: 0.149\t\t test accuracy: 0.967\n",
            "Epoch: [2510/5000]-----------------------\n",
            "training loss: 0.151\t\t training accuracy: 0.972\n",
            "test loss: 0.149\t\t test accuracy: 0.967\n",
            "Epoch: [2520/5000]-----------------------\n",
            "training loss: 0.151\t\t training accuracy: 0.972\n",
            "test loss: 0.148\t\t test accuracy: 0.967\n",
            "Epoch: [2530/5000]-----------------------\n",
            "training loss: 0.151\t\t training accuracy: 0.972\n",
            "test loss: 0.148\t\t test accuracy: 0.967\n",
            "Epoch: [2540/5000]-----------------------\n",
            "training loss: 0.151\t\t training accuracy: 0.972\n",
            "test loss: 0.148\t\t test accuracy: 0.967\n",
            "Epoch: [2550/5000]-----------------------\n",
            "training loss: 0.151\t\t training accuracy: 0.972\n",
            "test loss: 0.148\t\t test accuracy: 0.967\n",
            "Epoch: [2560/5000]-----------------------\n",
            "training loss: 0.150\t\t training accuracy: 0.972\n",
            "test loss: 0.147\t\t test accuracy: 0.967\n",
            "Epoch: [2570/5000]-----------------------\n",
            "training loss: 0.150\t\t training accuracy: 0.972\n",
            "test loss: 0.147\t\t test accuracy: 0.967\n",
            "Epoch: [2580/5000]-----------------------\n",
            "training loss: 0.150\t\t training accuracy: 0.972\n",
            "test loss: 0.147\t\t test accuracy: 0.967\n",
            "Epoch: [2590/5000]-----------------------\n",
            "training loss: 0.150\t\t training accuracy: 0.972\n",
            "test loss: 0.147\t\t test accuracy: 0.967\n",
            "Epoch: [2600/5000]-----------------------\n",
            "training loss: 0.149\t\t training accuracy: 0.972\n",
            "test loss: 0.147\t\t test accuracy: 0.967\n",
            "Epoch: [2610/5000]-----------------------\n",
            "training loss: 0.149\t\t training accuracy: 0.972\n",
            "test loss: 0.146\t\t test accuracy: 0.967\n",
            "Epoch: [2620/5000]-----------------------\n",
            "training loss: 0.149\t\t training accuracy: 0.972\n",
            "test loss: 0.146\t\t test accuracy: 0.967\n",
            "Epoch: [2630/5000]-----------------------\n",
            "training loss: 0.149\t\t training accuracy: 0.972\n",
            "test loss: 0.146\t\t test accuracy: 0.967\n",
            "Epoch: [2640/5000]-----------------------\n",
            "training loss: 0.149\t\t training accuracy: 0.972\n",
            "test loss: 0.146\t\t test accuracy: 0.967\n",
            "Epoch: [2650/5000]-----------------------\n",
            "training loss: 0.148\t\t training accuracy: 0.972\n",
            "test loss: 0.145\t\t test accuracy: 0.967\n",
            "Epoch: [2660/5000]-----------------------\n",
            "training loss: 0.148\t\t training accuracy: 0.972\n",
            "test loss: 0.145\t\t test accuracy: 0.967\n",
            "Epoch: [2670/5000]-----------------------\n",
            "training loss: 0.148\t\t training accuracy: 0.972\n",
            "test loss: 0.145\t\t test accuracy: 0.967\n",
            "Epoch: [2680/5000]-----------------------\n",
            "training loss: 0.148\t\t training accuracy: 0.972\n",
            "test loss: 0.145\t\t test accuracy: 0.967\n",
            "Epoch: [2690/5000]-----------------------\n",
            "training loss: 0.147\t\t training accuracy: 0.972\n",
            "test loss: 0.145\t\t test accuracy: 0.967\n",
            "Epoch: [2700/5000]-----------------------\n",
            "training loss: 0.147\t\t training accuracy: 0.972\n",
            "test loss: 0.144\t\t test accuracy: 0.967\n",
            "Epoch: [2710/5000]-----------------------\n",
            "training loss: 0.147\t\t training accuracy: 0.972\n",
            "test loss: 0.144\t\t test accuracy: 0.967\n",
            "Epoch: [2720/5000]-----------------------\n",
            "training loss: 0.147\t\t training accuracy: 0.972\n",
            "test loss: 0.144\t\t test accuracy: 0.967\n",
            "Epoch: [2730/5000]-----------------------\n",
            "training loss: 0.147\t\t training accuracy: 0.972\n",
            "test loss: 0.144\t\t test accuracy: 0.967\n",
            "Epoch: [2740/5000]-----------------------\n",
            "training loss: 0.146\t\t training accuracy: 0.972\n",
            "test loss: 0.144\t\t test accuracy: 0.967\n",
            "Epoch: [2750/5000]-----------------------\n",
            "training loss: 0.146\t\t training accuracy: 0.972\n",
            "test loss: 0.143\t\t test accuracy: 0.967\n",
            "Epoch: [2760/5000]-----------------------\n",
            "training loss: 0.146\t\t training accuracy: 0.972\n",
            "test loss: 0.143\t\t test accuracy: 0.967\n",
            "Epoch: [2770/5000]-----------------------\n",
            "training loss: 0.146\t\t training accuracy: 0.972\n",
            "test loss: 0.143\t\t test accuracy: 0.967\n",
            "Epoch: [2780/5000]-----------------------\n",
            "training loss: 0.146\t\t training accuracy: 0.972\n",
            "test loss: 0.143\t\t test accuracy: 0.967\n",
            "Epoch: [2790/5000]-----------------------\n",
            "training loss: 0.145\t\t training accuracy: 0.972\n",
            "test loss: 0.143\t\t test accuracy: 0.967\n",
            "Epoch: [2800/5000]-----------------------\n",
            "training loss: 0.145\t\t training accuracy: 0.972\n",
            "test loss: 0.142\t\t test accuracy: 0.967\n",
            "Epoch: [2810/5000]-----------------------\n",
            "training loss: 0.145\t\t training accuracy: 0.972\n",
            "test loss: 0.142\t\t test accuracy: 0.967\n",
            "Epoch: [2820/5000]-----------------------\n",
            "training loss: 0.145\t\t training accuracy: 0.972\n",
            "test loss: 0.142\t\t test accuracy: 0.967\n",
            "Epoch: [2830/5000]-----------------------\n",
            "training loss: 0.145\t\t training accuracy: 0.972\n",
            "test loss: 0.142\t\t test accuracy: 0.967\n",
            "Epoch: [2840/5000]-----------------------\n",
            "training loss: 0.144\t\t training accuracy: 0.972\n",
            "test loss: 0.142\t\t test accuracy: 0.967\n",
            "Epoch: [2850/5000]-----------------------\n",
            "training loss: 0.144\t\t training accuracy: 0.972\n",
            "test loss: 0.141\t\t test accuracy: 0.967\n",
            "Epoch: [2860/5000]-----------------------\n",
            "training loss: 0.144\t\t training accuracy: 0.972\n",
            "test loss: 0.141\t\t test accuracy: 0.967\n",
            "Epoch: [2870/5000]-----------------------\n",
            "training loss: 0.144\t\t training accuracy: 0.972\n",
            "test loss: 0.141\t\t test accuracy: 0.967\n",
            "Epoch: [2880/5000]-----------------------\n",
            "training loss: 0.144\t\t training accuracy: 0.972\n",
            "test loss: 0.141\t\t test accuracy: 0.967\n",
            "Epoch: [2890/5000]-----------------------\n",
            "training loss: 0.144\t\t training accuracy: 0.972\n",
            "test loss: 0.141\t\t test accuracy: 0.967\n",
            "Epoch: [2900/5000]-----------------------\n",
            "training loss: 0.143\t\t training accuracy: 0.972\n",
            "test loss: 0.141\t\t test accuracy: 0.967\n",
            "Epoch: [2910/5000]-----------------------\n",
            "training loss: 0.143\t\t training accuracy: 0.972\n",
            "test loss: 0.140\t\t test accuracy: 0.967\n",
            "Epoch: [2920/5000]-----------------------\n",
            "training loss: 0.143\t\t training accuracy: 0.972\n",
            "test loss: 0.140\t\t test accuracy: 0.967\n",
            "Epoch: [2930/5000]-----------------------\n",
            "training loss: 0.143\t\t training accuracy: 0.972\n",
            "test loss: 0.140\t\t test accuracy: 0.968\n",
            "Epoch: [2940/5000]-----------------------\n",
            "training loss: 0.143\t\t training accuracy: 0.972\n",
            "test loss: 0.140\t\t test accuracy: 0.968\n",
            "Epoch: [2950/5000]-----------------------\n",
            "training loss: 0.142\t\t training accuracy: 0.972\n",
            "test loss: 0.140\t\t test accuracy: 0.968\n",
            "Epoch: [2960/5000]-----------------------\n",
            "training loss: 0.142\t\t training accuracy: 0.972\n",
            "test loss: 0.139\t\t test accuracy: 0.968\n",
            "Epoch: [2970/5000]-----------------------\n",
            "training loss: 0.142\t\t training accuracy: 0.972\n",
            "test loss: 0.139\t\t test accuracy: 0.968\n",
            "Epoch: [2980/5000]-----------------------\n",
            "training loss: 0.142\t\t training accuracy: 0.972\n",
            "test loss: 0.139\t\t test accuracy: 0.968\n",
            "Epoch: [2990/5000]-----------------------\n",
            "training loss: 0.142\t\t training accuracy: 0.973\n",
            "test loss: 0.139\t\t test accuracy: 0.968\n",
            "Epoch: [3000/5000]-----------------------\n",
            "training loss: 0.142\t\t training accuracy: 0.973\n",
            "test loss: 0.139\t\t test accuracy: 0.968\n",
            "Epoch: [3010/5000]-----------------------\n",
            "training loss: 0.141\t\t training accuracy: 0.973\n",
            "test loss: 0.139\t\t test accuracy: 0.968\n",
            "Epoch: [3020/5000]-----------------------\n",
            "training loss: 0.141\t\t training accuracy: 0.973\n",
            "test loss: 0.138\t\t test accuracy: 0.968\n",
            "Epoch: [3030/5000]-----------------------\n",
            "training loss: 0.141\t\t training accuracy: 0.973\n",
            "test loss: 0.138\t\t test accuracy: 0.968\n",
            "Epoch: [3040/5000]-----------------------\n",
            "training loss: 0.141\t\t training accuracy: 0.973\n",
            "test loss: 0.138\t\t test accuracy: 0.968\n",
            "Epoch: [3050/5000]-----------------------\n",
            "training loss: 0.141\t\t training accuracy: 0.973\n",
            "test loss: 0.138\t\t test accuracy: 0.968\n",
            "Epoch: [3060/5000]-----------------------\n",
            "training loss: 0.141\t\t training accuracy: 0.973\n",
            "test loss: 0.138\t\t test accuracy: 0.968\n",
            "Epoch: [3070/5000]-----------------------\n",
            "training loss: 0.140\t\t training accuracy: 0.973\n",
            "test loss: 0.138\t\t test accuracy: 0.968\n",
            "Epoch: [3080/5000]-----------------------\n",
            "training loss: 0.140\t\t training accuracy: 0.973\n",
            "test loss: 0.137\t\t test accuracy: 0.968\n",
            "Epoch: [3090/5000]-----------------------\n",
            "training loss: 0.140\t\t training accuracy: 0.973\n",
            "test loss: 0.137\t\t test accuracy: 0.968\n",
            "Epoch: [3100/5000]-----------------------\n",
            "training loss: 0.140\t\t training accuracy: 0.973\n",
            "test loss: 0.137\t\t test accuracy: 0.968\n",
            "Epoch: [3110/5000]-----------------------\n",
            "training loss: 0.140\t\t training accuracy: 0.973\n",
            "test loss: 0.137\t\t test accuracy: 0.968\n",
            "Epoch: [3120/5000]-----------------------\n",
            "training loss: 0.140\t\t training accuracy: 0.973\n",
            "test loss: 0.137\t\t test accuracy: 0.968\n",
            "Epoch: [3130/5000]-----------------------\n",
            "training loss: 0.139\t\t training accuracy: 0.973\n",
            "test loss: 0.137\t\t test accuracy: 0.968\n",
            "Epoch: [3140/5000]-----------------------\n",
            "training loss: 0.139\t\t training accuracy: 0.973\n",
            "test loss: 0.136\t\t test accuracy: 0.968\n",
            "Epoch: [3150/5000]-----------------------\n",
            "training loss: 0.139\t\t training accuracy: 0.973\n",
            "test loss: 0.136\t\t test accuracy: 0.968\n",
            "Epoch: [3160/5000]-----------------------\n",
            "training loss: 0.139\t\t training accuracy: 0.973\n",
            "test loss: 0.136\t\t test accuracy: 0.968\n",
            "Epoch: [3170/5000]-----------------------\n",
            "training loss: 0.139\t\t training accuracy: 0.973\n",
            "test loss: 0.136\t\t test accuracy: 0.968\n",
            "Epoch: [3180/5000]-----------------------\n",
            "training loss: 0.139\t\t training accuracy: 0.973\n",
            "test loss: 0.136\t\t test accuracy: 0.968\n",
            "Epoch: [3190/5000]-----------------------\n",
            "training loss: 0.139\t\t training accuracy: 0.973\n",
            "test loss: 0.136\t\t test accuracy: 0.968\n",
            "Epoch: [3200/5000]-----------------------\n",
            "training loss: 0.138\t\t training accuracy: 0.973\n",
            "test loss: 0.136\t\t test accuracy: 0.968\n",
            "Epoch: [3210/5000]-----------------------\n",
            "training loss: 0.138\t\t training accuracy: 0.973\n",
            "test loss: 0.135\t\t test accuracy: 0.968\n",
            "Epoch: [3220/5000]-----------------------\n",
            "training loss: 0.138\t\t training accuracy: 0.973\n",
            "test loss: 0.135\t\t test accuracy: 0.968\n",
            "Epoch: [3230/5000]-----------------------\n",
            "training loss: 0.138\t\t training accuracy: 0.973\n",
            "test loss: 0.135\t\t test accuracy: 0.968\n",
            "Epoch: [3240/5000]-----------------------\n",
            "training loss: 0.138\t\t training accuracy: 0.973\n",
            "test loss: 0.135\t\t test accuracy: 0.968\n",
            "Epoch: [3250/5000]-----------------------\n",
            "training loss: 0.138\t\t training accuracy: 0.973\n",
            "test loss: 0.135\t\t test accuracy: 0.968\n",
            "Epoch: [3260/5000]-----------------------\n",
            "training loss: 0.137\t\t training accuracy: 0.973\n",
            "test loss: 0.135\t\t test accuracy: 0.968\n",
            "Epoch: [3270/5000]-----------------------\n",
            "training loss: 0.137\t\t training accuracy: 0.973\n",
            "test loss: 0.134\t\t test accuracy: 0.968\n",
            "Epoch: [3280/5000]-----------------------\n",
            "training loss: 0.137\t\t training accuracy: 0.973\n",
            "test loss: 0.134\t\t test accuracy: 0.968\n",
            "Epoch: [3290/5000]-----------------------\n",
            "training loss: 0.137\t\t training accuracy: 0.973\n",
            "test loss: 0.134\t\t test accuracy: 0.968\n",
            "Epoch: [3300/5000]-----------------------\n",
            "training loss: 0.137\t\t training accuracy: 0.973\n",
            "test loss: 0.134\t\t test accuracy: 0.968\n",
            "Epoch: [3310/5000]-----------------------\n",
            "training loss: 0.137\t\t training accuracy: 0.973\n",
            "test loss: 0.134\t\t test accuracy: 0.968\n",
            "Epoch: [3320/5000]-----------------------\n",
            "training loss: 0.137\t\t training accuracy: 0.973\n",
            "test loss: 0.134\t\t test accuracy: 0.968\n",
            "Epoch: [3330/5000]-----------------------\n",
            "training loss: 0.136\t\t training accuracy: 0.973\n",
            "test loss: 0.134\t\t test accuracy: 0.968\n",
            "Epoch: [3340/5000]-----------------------\n",
            "training loss: 0.136\t\t training accuracy: 0.973\n",
            "test loss: 0.133\t\t test accuracy: 0.968\n",
            "Epoch: [3350/5000]-----------------------\n",
            "training loss: 0.136\t\t training accuracy: 0.973\n",
            "test loss: 0.133\t\t test accuracy: 0.968\n",
            "Epoch: [3360/5000]-----------------------\n",
            "training loss: 0.136\t\t training accuracy: 0.973\n",
            "test loss: 0.133\t\t test accuracy: 0.968\n",
            "Epoch: [3370/5000]-----------------------\n",
            "training loss: 0.136\t\t training accuracy: 0.973\n",
            "test loss: 0.133\t\t test accuracy: 0.968\n",
            "Epoch: [3380/5000]-----------------------\n",
            "training loss: 0.136\t\t training accuracy: 0.973\n",
            "test loss: 0.133\t\t test accuracy: 0.968\n",
            "Epoch: [3390/5000]-----------------------\n",
            "training loss: 0.136\t\t training accuracy: 0.973\n",
            "test loss: 0.133\t\t test accuracy: 0.968\n",
            "Epoch: [3400/5000]-----------------------\n",
            "training loss: 0.136\t\t training accuracy: 0.973\n",
            "test loss: 0.133\t\t test accuracy: 0.968\n",
            "Epoch: [3410/5000]-----------------------\n",
            "training loss: 0.135\t\t training accuracy: 0.973\n",
            "test loss: 0.132\t\t test accuracy: 0.968\n",
            "Epoch: [3420/5000]-----------------------\n",
            "training loss: 0.135\t\t training accuracy: 0.973\n",
            "test loss: 0.132\t\t test accuracy: 0.968\n",
            "Epoch: [3430/5000]-----------------------\n",
            "training loss: 0.135\t\t training accuracy: 0.973\n",
            "test loss: 0.132\t\t test accuracy: 0.968\n",
            "Epoch: [3440/5000]-----------------------\n",
            "training loss: 0.135\t\t training accuracy: 0.973\n",
            "test loss: 0.132\t\t test accuracy: 0.968\n",
            "Epoch: [3450/5000]-----------------------\n",
            "training loss: 0.135\t\t training accuracy: 0.973\n",
            "test loss: 0.132\t\t test accuracy: 0.968\n",
            "Epoch: [3460/5000]-----------------------\n",
            "training loss: 0.135\t\t training accuracy: 0.973\n",
            "test loss: 0.132\t\t test accuracy: 0.968\n",
            "Epoch: [3470/5000]-----------------------\n",
            "training loss: 0.135\t\t training accuracy: 0.973\n",
            "test loss: 0.132\t\t test accuracy: 0.968\n",
            "Epoch: [3480/5000]-----------------------\n",
            "training loss: 0.134\t\t training accuracy: 0.973\n",
            "test loss: 0.132\t\t test accuracy: 0.968\n",
            "Epoch: [3490/5000]-----------------------\n",
            "training loss: 0.134\t\t training accuracy: 0.973\n",
            "test loss: 0.131\t\t test accuracy: 0.968\n",
            "Epoch: [3500/5000]-----------------------\n",
            "training loss: 0.134\t\t training accuracy: 0.973\n",
            "test loss: 0.131\t\t test accuracy: 0.968\n",
            "Epoch: [3510/5000]-----------------------\n",
            "training loss: 0.134\t\t training accuracy: 0.973\n",
            "test loss: 0.131\t\t test accuracy: 0.968\n",
            "Epoch: [3520/5000]-----------------------\n",
            "training loss: 0.134\t\t training accuracy: 0.973\n",
            "test loss: 0.131\t\t test accuracy: 0.968\n",
            "Epoch: [3530/5000]-----------------------\n",
            "training loss: 0.134\t\t training accuracy: 0.973\n",
            "test loss: 0.131\t\t test accuracy: 0.968\n",
            "Epoch: [3540/5000]-----------------------\n",
            "training loss: 0.134\t\t training accuracy: 0.973\n",
            "test loss: 0.131\t\t test accuracy: 0.968\n",
            "Epoch: [3550/5000]-----------------------\n",
            "training loss: 0.134\t\t training accuracy: 0.973\n",
            "test loss: 0.131\t\t test accuracy: 0.968\n",
            "Epoch: [3560/5000]-----------------------\n",
            "training loss: 0.133\t\t training accuracy: 0.973\n",
            "test loss: 0.131\t\t test accuracy: 0.968\n",
            "Epoch: [3570/5000]-----------------------\n",
            "training loss: 0.133\t\t training accuracy: 0.973\n",
            "test loss: 0.130\t\t test accuracy: 0.968\n",
            "Epoch: [3580/5000]-----------------------\n",
            "training loss: 0.133\t\t training accuracy: 0.973\n",
            "test loss: 0.130\t\t test accuracy: 0.968\n",
            "Epoch: [3590/5000]-----------------------\n",
            "training loss: 0.133\t\t training accuracy: 0.973\n",
            "test loss: 0.130\t\t test accuracy: 0.968\n",
            "Epoch: [3600/5000]-----------------------\n",
            "training loss: 0.133\t\t training accuracy: 0.973\n",
            "test loss: 0.130\t\t test accuracy: 0.968\n",
            "Epoch: [3610/5000]-----------------------\n",
            "training loss: 0.133\t\t training accuracy: 0.973\n",
            "test loss: 0.130\t\t test accuracy: 0.968\n",
            "Epoch: [3620/5000]-----------------------\n",
            "training loss: 0.133\t\t training accuracy: 0.973\n",
            "test loss: 0.130\t\t test accuracy: 0.968\n",
            "Epoch: [3630/5000]-----------------------\n",
            "training loss: 0.133\t\t training accuracy: 0.973\n",
            "test loss: 0.130\t\t test accuracy: 0.968\n",
            "Epoch: [3640/5000]-----------------------\n",
            "training loss: 0.132\t\t training accuracy: 0.973\n",
            "test loss: 0.130\t\t test accuracy: 0.968\n",
            "Epoch: [3650/5000]-----------------------\n",
            "training loss: 0.132\t\t training accuracy: 0.973\n",
            "test loss: 0.129\t\t test accuracy: 0.968\n",
            "Epoch: [3660/5000]-----------------------\n",
            "training loss: 0.132\t\t training accuracy: 0.973\n",
            "test loss: 0.129\t\t test accuracy: 0.968\n",
            "Epoch: [3670/5000]-----------------------\n",
            "training loss: 0.132\t\t training accuracy: 0.973\n",
            "test loss: 0.129\t\t test accuracy: 0.968\n",
            "Epoch: [3680/5000]-----------------------\n",
            "training loss: 0.132\t\t training accuracy: 0.973\n",
            "test loss: 0.129\t\t test accuracy: 0.968\n",
            "Epoch: [3690/5000]-----------------------\n",
            "training loss: 0.132\t\t training accuracy: 0.973\n",
            "test loss: 0.129\t\t test accuracy: 0.968\n",
            "Epoch: [3700/5000]-----------------------\n",
            "training loss: 0.132\t\t training accuracy: 0.973\n",
            "test loss: 0.129\t\t test accuracy: 0.968\n",
            "Epoch: [3710/5000]-----------------------\n",
            "training loss: 0.132\t\t training accuracy: 0.973\n",
            "test loss: 0.129\t\t test accuracy: 0.968\n",
            "Epoch: [3720/5000]-----------------------\n",
            "training loss: 0.131\t\t training accuracy: 0.973\n",
            "test loss: 0.129\t\t test accuracy: 0.968\n",
            "Epoch: [3730/5000]-----------------------\n",
            "training loss: 0.131\t\t training accuracy: 0.973\n",
            "test loss: 0.128\t\t test accuracy: 0.968\n",
            "Epoch: [3740/5000]-----------------------\n",
            "training loss: 0.131\t\t training accuracy: 0.973\n",
            "test loss: 0.128\t\t test accuracy: 0.968\n",
            "Epoch: [3750/5000]-----------------------\n",
            "training loss: 0.131\t\t training accuracy: 0.973\n",
            "test loss: 0.128\t\t test accuracy: 0.968\n",
            "Epoch: [3760/5000]-----------------------\n",
            "training loss: 0.131\t\t training accuracy: 0.973\n",
            "test loss: 0.128\t\t test accuracy: 0.968\n",
            "Epoch: [3770/5000]-----------------------\n",
            "training loss: 0.131\t\t training accuracy: 0.973\n",
            "test loss: 0.128\t\t test accuracy: 0.969\n",
            "Epoch: [3780/5000]-----------------------\n",
            "training loss: 0.131\t\t training accuracy: 0.973\n",
            "test loss: 0.128\t\t test accuracy: 0.969\n",
            "Epoch: [3790/5000]-----------------------\n",
            "training loss: 0.131\t\t training accuracy: 0.973\n",
            "test loss: 0.128\t\t test accuracy: 0.969\n",
            "Epoch: [3800/5000]-----------------------\n",
            "training loss: 0.131\t\t training accuracy: 0.973\n",
            "test loss: 0.128\t\t test accuracy: 0.969\n",
            "Epoch: [3810/5000]-----------------------\n",
            "training loss: 0.130\t\t training accuracy: 0.973\n",
            "test loss: 0.128\t\t test accuracy: 0.969\n",
            "Epoch: [3820/5000]-----------------------\n",
            "training loss: 0.130\t\t training accuracy: 0.973\n",
            "test loss: 0.127\t\t test accuracy: 0.969\n",
            "Epoch: [3830/5000]-----------------------\n",
            "training loss: 0.130\t\t training accuracy: 0.973\n",
            "test loss: 0.127\t\t test accuracy: 0.969\n",
            "Epoch: [3840/5000]-----------------------\n",
            "training loss: 0.130\t\t training accuracy: 0.973\n",
            "test loss: 0.127\t\t test accuracy: 0.969\n",
            "Epoch: [3850/5000]-----------------------\n",
            "training loss: 0.130\t\t training accuracy: 0.973\n",
            "test loss: 0.127\t\t test accuracy: 0.969\n",
            "Epoch: [3860/5000]-----------------------\n",
            "training loss: 0.130\t\t training accuracy: 0.973\n",
            "test loss: 0.127\t\t test accuracy: 0.969\n",
            "Epoch: [3870/5000]-----------------------\n",
            "training loss: 0.130\t\t training accuracy: 0.973\n",
            "test loss: 0.127\t\t test accuracy: 0.969\n",
            "Epoch: [3880/5000]-----------------------\n",
            "training loss: 0.130\t\t training accuracy: 0.973\n",
            "test loss: 0.127\t\t test accuracy: 0.969\n",
            "Epoch: [3890/5000]-----------------------\n",
            "training loss: 0.130\t\t training accuracy: 0.973\n",
            "test loss: 0.127\t\t test accuracy: 0.969\n",
            "Epoch: [3900/5000]-----------------------\n",
            "training loss: 0.129\t\t training accuracy: 0.973\n",
            "test loss: 0.127\t\t test accuracy: 0.969\n",
            "Epoch: [3910/5000]-----------------------\n",
            "training loss: 0.129\t\t training accuracy: 0.973\n",
            "test loss: 0.126\t\t test accuracy: 0.969\n",
            "Epoch: [3920/5000]-----------------------\n",
            "training loss: 0.129\t\t training accuracy: 0.973\n",
            "test loss: 0.126\t\t test accuracy: 0.969\n",
            "Epoch: [3930/5000]-----------------------\n",
            "training loss: 0.129\t\t training accuracy: 0.973\n",
            "test loss: 0.126\t\t test accuracy: 0.969\n",
            "Epoch: [3940/5000]-----------------------\n",
            "training loss: 0.129\t\t training accuracy: 0.973\n",
            "test loss: 0.126\t\t test accuracy: 0.969\n",
            "Epoch: [3950/5000]-----------------------\n",
            "training loss: 0.129\t\t training accuracy: 0.973\n",
            "test loss: 0.126\t\t test accuracy: 0.969\n",
            "Epoch: [3960/5000]-----------------------\n",
            "training loss: 0.129\t\t training accuracy: 0.973\n",
            "test loss: 0.126\t\t test accuracy: 0.969\n",
            "Epoch: [3970/5000]-----------------------\n",
            "training loss: 0.129\t\t training accuracy: 0.973\n",
            "test loss: 0.126\t\t test accuracy: 0.969\n",
            "Epoch: [3980/5000]-----------------------\n",
            "training loss: 0.129\t\t training accuracy: 0.973\n",
            "test loss: 0.126\t\t test accuracy: 0.969\n",
            "Epoch: [3990/5000]-----------------------\n",
            "training loss: 0.129\t\t training accuracy: 0.974\n",
            "test loss: 0.126\t\t test accuracy: 0.969\n",
            "Epoch: [4000/5000]-----------------------\n",
            "training loss: 0.128\t\t training accuracy: 0.974\n",
            "test loss: 0.125\t\t test accuracy: 0.969\n",
            "Epoch: [4010/5000]-----------------------\n",
            "training loss: 0.128\t\t training accuracy: 0.974\n",
            "test loss: 0.125\t\t test accuracy: 0.969\n",
            "Epoch: [4020/5000]-----------------------\n",
            "training loss: 0.128\t\t training accuracy: 0.974\n",
            "test loss: 0.125\t\t test accuracy: 0.969\n",
            "Epoch: [4030/5000]-----------------------\n",
            "training loss: 0.128\t\t training accuracy: 0.974\n",
            "test loss: 0.125\t\t test accuracy: 0.969\n",
            "Epoch: [4040/5000]-----------------------\n",
            "training loss: 0.128\t\t training accuracy: 0.974\n",
            "test loss: 0.125\t\t test accuracy: 0.969\n",
            "Epoch: [4050/5000]-----------------------\n",
            "training loss: 0.128\t\t training accuracy: 0.974\n",
            "test loss: 0.125\t\t test accuracy: 0.969\n",
            "Epoch: [4060/5000]-----------------------\n",
            "training loss: 0.128\t\t training accuracy: 0.974\n",
            "test loss: 0.125\t\t test accuracy: 0.969\n",
            "Epoch: [4070/5000]-----------------------\n",
            "training loss: 0.128\t\t training accuracy: 0.974\n",
            "test loss: 0.125\t\t test accuracy: 0.969\n",
            "Epoch: [4080/5000]-----------------------\n",
            "training loss: 0.128\t\t training accuracy: 0.974\n",
            "test loss: 0.125\t\t test accuracy: 0.969\n",
            "Epoch: [4090/5000]-----------------------\n",
            "training loss: 0.128\t\t training accuracy: 0.974\n",
            "test loss: 0.125\t\t test accuracy: 0.969\n",
            "Epoch: [4100/5000]-----------------------\n",
            "training loss: 0.127\t\t training accuracy: 0.974\n",
            "test loss: 0.124\t\t test accuracy: 0.969\n",
            "Epoch: [4110/5000]-----------------------\n",
            "training loss: 0.127\t\t training accuracy: 0.974\n",
            "test loss: 0.124\t\t test accuracy: 0.969\n",
            "Epoch: [4120/5000]-----------------------\n",
            "training loss: 0.127\t\t training accuracy: 0.974\n",
            "test loss: 0.124\t\t test accuracy: 0.969\n",
            "Epoch: [4130/5000]-----------------------\n",
            "training loss: 0.127\t\t training accuracy: 0.974\n",
            "test loss: 0.124\t\t test accuracy: 0.969\n",
            "Epoch: [4140/5000]-----------------------\n",
            "training loss: 0.127\t\t training accuracy: 0.974\n",
            "test loss: 0.124\t\t test accuracy: 0.969\n",
            "Epoch: [4150/5000]-----------------------\n",
            "training loss: 0.127\t\t training accuracy: 0.974\n",
            "test loss: 0.124\t\t test accuracy: 0.969\n",
            "Epoch: [4160/5000]-----------------------\n",
            "training loss: 0.127\t\t training accuracy: 0.974\n",
            "test loss: 0.124\t\t test accuracy: 0.969\n",
            "Epoch: [4170/5000]-----------------------\n",
            "training loss: 0.127\t\t training accuracy: 0.974\n",
            "test loss: 0.124\t\t test accuracy: 0.969\n",
            "Epoch: [4180/5000]-----------------------\n",
            "training loss: 0.127\t\t training accuracy: 0.974\n",
            "test loss: 0.124\t\t test accuracy: 0.969\n",
            "Epoch: [4190/5000]-----------------------\n",
            "training loss: 0.127\t\t training accuracy: 0.974\n",
            "test loss: 0.124\t\t test accuracy: 0.969\n",
            "Epoch: [4200/5000]-----------------------\n",
            "training loss: 0.126\t\t training accuracy: 0.974\n",
            "test loss: 0.123\t\t test accuracy: 0.969\n",
            "Epoch: [4210/5000]-----------------------\n",
            "training loss: 0.126\t\t training accuracy: 0.974\n",
            "test loss: 0.123\t\t test accuracy: 0.969\n",
            "Epoch: [4220/5000]-----------------------\n",
            "training loss: 0.126\t\t training accuracy: 0.974\n",
            "test loss: 0.123\t\t test accuracy: 0.969\n",
            "Epoch: [4230/5000]-----------------------\n",
            "training loss: 0.126\t\t training accuracy: 0.974\n",
            "test loss: 0.123\t\t test accuracy: 0.969\n",
            "Epoch: [4240/5000]-----------------------\n",
            "training loss: 0.126\t\t training accuracy: 0.974\n",
            "test loss: 0.123\t\t test accuracy: 0.969\n",
            "Epoch: [4250/5000]-----------------------\n",
            "training loss: 0.126\t\t training accuracy: 0.974\n",
            "test loss: 0.123\t\t test accuracy: 0.969\n",
            "Epoch: [4260/5000]-----------------------\n",
            "training loss: 0.126\t\t training accuracy: 0.974\n",
            "test loss: 0.123\t\t test accuracy: 0.969\n",
            "Epoch: [4270/5000]-----------------------\n",
            "training loss: 0.126\t\t training accuracy: 0.974\n",
            "test loss: 0.123\t\t test accuracy: 0.969\n",
            "Epoch: [4280/5000]-----------------------\n",
            "training loss: 0.126\t\t training accuracy: 0.974\n",
            "test loss: 0.123\t\t test accuracy: 0.969\n",
            "Epoch: [4290/5000]-----------------------\n",
            "training loss: 0.126\t\t training accuracy: 0.974\n",
            "test loss: 0.123\t\t test accuracy: 0.969\n",
            "Epoch: [4300/5000]-----------------------\n",
            "training loss: 0.126\t\t training accuracy: 0.974\n",
            "test loss: 0.123\t\t test accuracy: 0.969\n",
            "Epoch: [4310/5000]-----------------------\n",
            "training loss: 0.125\t\t training accuracy: 0.974\n",
            "test loss: 0.122\t\t test accuracy: 0.969\n",
            "Epoch: [4320/5000]-----------------------\n",
            "training loss: 0.125\t\t training accuracy: 0.974\n",
            "test loss: 0.122\t\t test accuracy: 0.969\n",
            "Epoch: [4330/5000]-----------------------\n",
            "training loss: 0.125\t\t training accuracy: 0.974\n",
            "test loss: 0.122\t\t test accuracy: 0.969\n",
            "Epoch: [4340/5000]-----------------------\n",
            "training loss: 0.125\t\t training accuracy: 0.974\n",
            "test loss: 0.122\t\t test accuracy: 0.969\n",
            "Epoch: [4350/5000]-----------------------\n",
            "training loss: 0.125\t\t training accuracy: 0.974\n",
            "test loss: 0.122\t\t test accuracy: 0.969\n",
            "Epoch: [4360/5000]-----------------------\n",
            "training loss: 0.125\t\t training accuracy: 0.974\n",
            "test loss: 0.122\t\t test accuracy: 0.969\n",
            "Epoch: [4370/5000]-----------------------\n",
            "training loss: 0.125\t\t training accuracy: 0.974\n",
            "test loss: 0.122\t\t test accuracy: 0.969\n",
            "Epoch: [4380/5000]-----------------------\n",
            "training loss: 0.125\t\t training accuracy: 0.974\n",
            "test loss: 0.122\t\t test accuracy: 0.969\n",
            "Epoch: [4390/5000]-----------------------\n",
            "training loss: 0.125\t\t training accuracy: 0.974\n",
            "test loss: 0.122\t\t test accuracy: 0.969\n",
            "Epoch: [4400/5000]-----------------------\n",
            "training loss: 0.125\t\t training accuracy: 0.974\n",
            "test loss: 0.122\t\t test accuracy: 0.969\n",
            "Epoch: [4410/5000]-----------------------\n",
            "training loss: 0.125\t\t training accuracy: 0.974\n",
            "test loss: 0.122\t\t test accuracy: 0.969\n",
            "Epoch: [4420/5000]-----------------------\n",
            "training loss: 0.124\t\t training accuracy: 0.974\n",
            "test loss: 0.121\t\t test accuracy: 0.969\n",
            "Epoch: [4430/5000]-----------------------\n",
            "training loss: 0.124\t\t training accuracy: 0.974\n",
            "test loss: 0.121\t\t test accuracy: 0.969\n",
            "Epoch: [4440/5000]-----------------------\n",
            "training loss: 0.124\t\t training accuracy: 0.974\n",
            "test loss: 0.121\t\t test accuracy: 0.969\n",
            "Epoch: [4450/5000]-----------------------\n",
            "training loss: 0.124\t\t training accuracy: 0.974\n",
            "test loss: 0.121\t\t test accuracy: 0.969\n",
            "Epoch: [4460/5000]-----------------------\n",
            "training loss: 0.124\t\t training accuracy: 0.974\n",
            "test loss: 0.121\t\t test accuracy: 0.969\n",
            "Epoch: [4470/5000]-----------------------\n",
            "training loss: 0.124\t\t training accuracy: 0.974\n",
            "test loss: 0.121\t\t test accuracy: 0.969\n",
            "Epoch: [4480/5000]-----------------------\n",
            "training loss: 0.124\t\t training accuracy: 0.974\n",
            "test loss: 0.121\t\t test accuracy: 0.969\n",
            "Epoch: [4490/5000]-----------------------\n",
            "training loss: 0.124\t\t training accuracy: 0.974\n",
            "test loss: 0.121\t\t test accuracy: 0.969\n",
            "Epoch: [4500/5000]-----------------------\n",
            "training loss: 0.124\t\t training accuracy: 0.974\n",
            "test loss: 0.121\t\t test accuracy: 0.969\n",
            "Epoch: [4510/5000]-----------------------\n",
            "training loss: 0.124\t\t training accuracy: 0.974\n",
            "test loss: 0.121\t\t test accuracy: 0.969\n",
            "Epoch: [4520/5000]-----------------------\n",
            "training loss: 0.124\t\t training accuracy: 0.974\n",
            "test loss: 0.121\t\t test accuracy: 0.969\n",
            "Epoch: [4530/5000]-----------------------\n",
            "training loss: 0.124\t\t training accuracy: 0.974\n",
            "test loss: 0.121\t\t test accuracy: 0.969\n",
            "Epoch: [4540/5000]-----------------------\n",
            "training loss: 0.123\t\t training accuracy: 0.974\n",
            "test loss: 0.120\t\t test accuracy: 0.969\n",
            "Epoch: [4550/5000]-----------------------\n",
            "training loss: 0.123\t\t training accuracy: 0.974\n",
            "test loss: 0.120\t\t test accuracy: 0.969\n",
            "Epoch: [4560/5000]-----------------------\n",
            "training loss: 0.123\t\t training accuracy: 0.974\n",
            "test loss: 0.120\t\t test accuracy: 0.969\n",
            "Epoch: [4570/5000]-----------------------\n",
            "training loss: 0.123\t\t training accuracy: 0.974\n",
            "test loss: 0.120\t\t test accuracy: 0.969\n",
            "Epoch: [4580/5000]-----------------------\n",
            "training loss: 0.123\t\t training accuracy: 0.974\n",
            "test loss: 0.120\t\t test accuracy: 0.969\n",
            "Epoch: [4590/5000]-----------------------\n",
            "training loss: 0.123\t\t training accuracy: 0.974\n",
            "test loss: 0.120\t\t test accuracy: 0.969\n",
            "Epoch: [4600/5000]-----------------------\n",
            "training loss: 0.123\t\t training accuracy: 0.974\n",
            "test loss: 0.120\t\t test accuracy: 0.969\n",
            "Epoch: [4610/5000]-----------------------\n",
            "training loss: 0.123\t\t training accuracy: 0.974\n",
            "test loss: 0.120\t\t test accuracy: 0.969\n",
            "Epoch: [4620/5000]-----------------------\n",
            "training loss: 0.123\t\t training accuracy: 0.974\n",
            "test loss: 0.120\t\t test accuracy: 0.969\n",
            "Epoch: [4630/5000]-----------------------\n",
            "training loss: 0.123\t\t training accuracy: 0.974\n",
            "test loss: 0.120\t\t test accuracy: 0.969\n",
            "Epoch: [4640/5000]-----------------------\n",
            "training loss: 0.123\t\t training accuracy: 0.974\n",
            "test loss: 0.120\t\t test accuracy: 0.969\n",
            "Epoch: [4650/5000]-----------------------\n",
            "training loss: 0.123\t\t training accuracy: 0.974\n",
            "test loss: 0.120\t\t test accuracy: 0.969\n",
            "Epoch: [4660/5000]-----------------------\n",
            "training loss: 0.123\t\t training accuracy: 0.974\n",
            "test loss: 0.119\t\t test accuracy: 0.969\n",
            "Epoch: [4670/5000]-----------------------\n",
            "training loss: 0.122\t\t training accuracy: 0.974\n",
            "test loss: 0.119\t\t test accuracy: 0.969\n",
            "Epoch: [4680/5000]-----------------------\n",
            "training loss: 0.122\t\t training accuracy: 0.974\n",
            "test loss: 0.119\t\t test accuracy: 0.969\n",
            "Epoch: [4690/5000]-----------------------\n",
            "training loss: 0.122\t\t training accuracy: 0.974\n",
            "test loss: 0.119\t\t test accuracy: 0.969\n",
            "Epoch: [4700/5000]-----------------------\n",
            "training loss: 0.122\t\t training accuracy: 0.974\n",
            "test loss: 0.119\t\t test accuracy: 0.969\n",
            "Epoch: [4710/5000]-----------------------\n",
            "training loss: 0.122\t\t training accuracy: 0.974\n",
            "test loss: 0.119\t\t test accuracy: 0.969\n",
            "Epoch: [4720/5000]-----------------------\n",
            "training loss: 0.122\t\t training accuracy: 0.974\n",
            "test loss: 0.119\t\t test accuracy: 0.969\n",
            "Epoch: [4730/5000]-----------------------\n",
            "training loss: 0.122\t\t training accuracy: 0.974\n",
            "test loss: 0.119\t\t test accuracy: 0.969\n",
            "Epoch: [4740/5000]-----------------------\n",
            "training loss: 0.122\t\t training accuracy: 0.974\n",
            "test loss: 0.119\t\t test accuracy: 0.969\n",
            "Epoch: [4750/5000]-----------------------\n",
            "training loss: 0.122\t\t training accuracy: 0.974\n",
            "test loss: 0.119\t\t test accuracy: 0.969\n",
            "Epoch: [4760/5000]-----------------------\n",
            "training loss: 0.122\t\t training accuracy: 0.974\n",
            "test loss: 0.119\t\t test accuracy: 0.969\n",
            "Epoch: [4770/5000]-----------------------\n",
            "training loss: 0.122\t\t training accuracy: 0.974\n",
            "test loss: 0.119\t\t test accuracy: 0.969\n",
            "Epoch: [4780/5000]-----------------------\n",
            "training loss: 0.122\t\t training accuracy: 0.974\n",
            "test loss: 0.119\t\t test accuracy: 0.969\n",
            "Epoch: [4790/5000]-----------------------\n",
            "training loss: 0.122\t\t training accuracy: 0.974\n",
            "test loss: 0.118\t\t test accuracy: 0.969\n",
            "Epoch: [4800/5000]-----------------------\n",
            "training loss: 0.121\t\t training accuracy: 0.974\n",
            "test loss: 0.118\t\t test accuracy: 0.969\n",
            "Epoch: [4810/5000]-----------------------\n",
            "training loss: 0.121\t\t training accuracy: 0.974\n",
            "test loss: 0.118\t\t test accuracy: 0.969\n",
            "Epoch: [4820/5000]-----------------------\n",
            "training loss: 0.121\t\t training accuracy: 0.974\n",
            "test loss: 0.118\t\t test accuracy: 0.969\n",
            "Epoch: [4830/5000]-----------------------\n",
            "training loss: 0.121\t\t training accuracy: 0.974\n",
            "test loss: 0.118\t\t test accuracy: 0.969\n",
            "Epoch: [4840/5000]-----------------------\n",
            "training loss: 0.121\t\t training accuracy: 0.974\n",
            "test loss: 0.118\t\t test accuracy: 0.969\n",
            "Epoch: [4850/5000]-----------------------\n",
            "training loss: 0.121\t\t training accuracy: 0.974\n",
            "test loss: 0.118\t\t test accuracy: 0.969\n",
            "Epoch: [4860/5000]-----------------------\n",
            "training loss: 0.121\t\t training accuracy: 0.974\n",
            "test loss: 0.118\t\t test accuracy: 0.969\n",
            "Epoch: [4870/5000]-----------------------\n",
            "training loss: 0.121\t\t training accuracy: 0.974\n",
            "test loss: 0.118\t\t test accuracy: 0.969\n",
            "Epoch: [4880/5000]-----------------------\n",
            "training loss: 0.121\t\t training accuracy: 0.974\n",
            "test loss: 0.118\t\t test accuracy: 0.969\n",
            "Epoch: [4890/5000]-----------------------\n",
            "training loss: 0.121\t\t training accuracy: 0.974\n",
            "test loss: 0.118\t\t test accuracy: 0.969\n",
            "Epoch: [4900/5000]-----------------------\n",
            "training loss: 0.121\t\t training accuracy: 0.974\n",
            "test loss: 0.118\t\t test accuracy: 0.969\n",
            "Epoch: [4910/5000]-----------------------\n",
            "training loss: 0.121\t\t training accuracy: 0.974\n",
            "test loss: 0.118\t\t test accuracy: 0.969\n",
            "Epoch: [4920/5000]-----------------------\n",
            "training loss: 0.121\t\t training accuracy: 0.974\n",
            "test loss: 0.118\t\t test accuracy: 0.969\n",
            "Epoch: [4930/5000]-----------------------\n",
            "training loss: 0.121\t\t training accuracy: 0.974\n",
            "test loss: 0.117\t\t test accuracy: 0.969\n",
            "Epoch: [4940/5000]-----------------------\n",
            "training loss: 0.120\t\t training accuracy: 0.974\n",
            "test loss: 0.117\t\t test accuracy: 0.969\n",
            "Epoch: [4950/5000]-----------------------\n",
            "training loss: 0.120\t\t training accuracy: 0.974\n",
            "test loss: 0.117\t\t test accuracy: 0.969\n",
            "Epoch: [4960/5000]-----------------------\n",
            "training loss: 0.120\t\t training accuracy: 0.974\n",
            "test loss: 0.117\t\t test accuracy: 0.969\n",
            "Epoch: [4970/5000]-----------------------\n",
            "training loss: 0.120\t\t training accuracy: 0.974\n",
            "test loss: 0.117\t\t test accuracy: 0.969\n",
            "Epoch: [4980/5000]-----------------------\n",
            "training loss: 0.120\t\t training accuracy: 0.974\n",
            "test loss: 0.117\t\t test accuracy: 0.969\n",
            "Epoch: [4990/5000]-----------------------\n",
            "training loss: 0.120\t\t training accuracy: 0.974\n",
            "test loss: 0.117\t\t test accuracy: 0.969\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.plot(range(n_epochs), train_losses, label = \"Training loss\")\n",
        "plt.plot(range(n_epochs), test_losses, label = \"Testing loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Log loss\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "ZkPBgfB4N7DE",
        "outputId": "704f22ca-b665-4b57-b98e-677e3c871a2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f65bf115a90>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(n_epochs), train_accuracy, label = \"Training accuracy\")\n",
        "plt.plot(range(n_epochs), test_accuracy, label = \"Testing accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.legend()\n",
        "plt.title(\"Accuracy vs training iterations\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "v-018VkmOKBZ",
        "outputId": "ae234b01-b609-4c7b-9912-2cbc1335f89b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Accuracy vs training iterations')"
            ]
          },
          "metadata": {},
          "execution_count": 47
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References:\n",
        "1. https://www.tensorflow.org/guide/core/logistic_regression_core\n",
        "2. https://builtin.com/data-science/guide-logistic-regression-tensorflow-20\n",
        "3. https://adventuresinmachinelearning.com/python-tensorflow-tutorial/\n",
        "4. https://www.ritchieng.com/machine-learning/deep-learning/tensorflow/regularization/\n",
        "5. https://neuraspike.com/blog/l2-regularization-with-python/\n"
      ],
      "metadata": {
        "id": "SGSyajQUgZ-S"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}